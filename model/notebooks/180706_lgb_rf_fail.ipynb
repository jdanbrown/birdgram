{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How big are lgb trees in rf mode? Any smaller than sk trees?\n",
    "#   - Abort! -- doesn't support multiclass output (see bottom)\n",
    "#   - [-] Figure out repr to evaluate model_size ~ (n_species, n_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lgb reference\n",
    "#   - https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst\n",
    "#   - https://github.com/Microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py\n",
    "#   - https://github.com/Microsoft/LightGBM/blob/master/docs/Features.rst\n",
    "#   - https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    "#   - https://sites.google.com/view/lauraepp/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((100, 10), (100,))"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "text/plain": "array(['f', 'q', 'g', 'b', 'l'], dtype='<U1')"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "text/plain": "((80, 10), (80,))"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "text/plain": "((20, 10), (20,))"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    }
   ],
   "source": [
    "# Generate data\n",
    "random_state = np.random.RandomState(0)\n",
    "n = 100\n",
    "f = 10\n",
    "X = random_state.rand(n, f)\n",
    "classes = list(string.ascii_lowercase)\n",
    "y = np_sample(classes, n=n, replace=True)\n",
    "yi = lambda y: np.array([classes.index(_y) for _y in y])  # lgb api wants num labels, not str labels\n",
    "(X_train, X_test, y_train, y_test) = sk.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "display(\n",
    "    (X.shape, y.shape),\n",
    "    y[:5],\n",
    "    (X_train.shape, y_train.shape),\n",
    "    (X_test.shape, y_test.shape),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lgb data\n",
    "lgb_train = lgb.Dataset(X_train, yi(y_train), free_raw_data=False)\n",
    "lgb_test = lgb.Dataset(X_test, yi(y_test), free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train_kwargs = dict(\n",
    "    # lgb.train args that feel like they belong in lgb_params instead\n",
    "\n",
    "    num_boost_round=10,\n",
    "    #   - Default: 100\n",
    "    #   - \"Note: internally, LightGBM constructs num_class * num_iterations trees for multi-class classification problems\"\n",
    "\n",
    "    # early_stopping_rounds=None,\n",
    "    #   - Default: None (disabled)\n",
    "    #   - \"Activates early stopping. The model will train until the validation score stops improving.\"\n",
    "    #   - \"Requires at least one validation data and one metric. If there's more than one, will check all of them except\n",
    "    #     the training data.\"\n",
    "    #   - \"If early stopping occurs, the model will add ``best_iteration`` field\"\n",
    "\n",
    "    # learning_rates=None,\n",
    "    #   - Default: None\n",
    "    #   - Dynamic learning rate\n",
    "    #   - learning_rates: list, callable or None, optional (default=None)\n",
    "    #       List of learning rates for each boosting round or a customized function that calculates ``learning_rate``\n",
    "    #       in terms of current number of round (e.g. yields learning rate decay).\n",
    "\n",
    ")\n",
    "lgb_params = dict(\n",
    "    # https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst\n",
    "\n",
    "    # Core Parameters\n",
    "    objective='multiclass',\n",
    "    #   - Must also set num_class\n",
    "    # boosting='gbdt',  # Default\n",
    "    boosting='rf',\n",
    "    #   - XXX Ah crap, rf mode doesn't allow multiclass:\n",
    "    #       - https://github.com/Microsoft/LightGBM/blob/v2.1.2/src/boosting/rf.hpp#L40\n",
    "    #       - https://github.com/Microsoft/LightGBM/issues/881\n",
    "    #   - References for rf mode\n",
    "    #       - https://github.com/Microsoft/LightGBM/blob/master/docs/FAQ.rst -> search \"forest\"\n",
    "    #       - https://github.com/Microsoft/LightGBM/issues/691\n",
    "    #       - Discussion on sampling without (lgb rf) vs. with (typical rf) replacement:\n",
    "    #           - https://github.com/Microsoft/LightGBM/pull/884\n",
    "    #           - https://github.com/Microsoft/LightGBM/issues/883\n",
    "    #           - https://github.com/Microsoft/LightGBM/issues/1038\n",
    "    #       - https://github.com/Microsoft/LightGBM/issues/47\n",
    "    #       - https://github.com/Microsoft/LightGBM/issues/1431\n",
    "    #       - Code\n",
    "    #           - https://github.com/Microsoft/LightGBM/pull/678\n",
    "    #           - https://github.com/Microsoft/LightGBM/blob/v2.1.2/src/boosting/rf.hpp\n",
    "    # num_iterations=100,\n",
    "    #   - [Deprecated: moved to .train]\n",
    "    # learning_rate=.01,  # Default\n",
    "    #   - Overridden to 1.0 in rf mode [https://github.com/Microsoft/LightGBM/blob/v2.1.2/src/boosting/rf.hpp#L43]\n",
    "    # num_leaves=31,\n",
    "    # tree_learner='serial',\n",
    "    #   - Single machine, multicore: 'serial' (default)\n",
    "    #   - [ignore] Distributed training: 'feature' | 'data' | 'voting'\n",
    "    #   - https://github.com/Microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.rst\n",
    "    # num_threads=0,\n",
    "    # device_type='cpu',\n",
    "    #   - 'cpu' (default) | 'gpu'\n",
    "    #   - Would gpu need rebuild? https://github.com/Microsoft/LightGBM/blob/master/docs/Installation-Guide.rst#build-gpu-version\n",
    "    #   - \"Note: it is recommended to use the smaller max_bin (e.g. 63) to get the better speed up\" (below)\n",
    "    #   - \"Note: for the faster speed, GPU uses 32-bit float point to sum up by default, so this may affect the\n",
    "    #     accuracy for some tasks. You can set gpu_use_dp=true to enable 64-bit float point, but it will slow down\n",
    "    #     the training\"\n",
    "    seed=0,\n",
    "    #   - Generates all seeds (e.g. data_random_seed)\n",
    "\n",
    "    # Learning Control Parameters\n",
    "    # max_depth=-1,\n",
    "    #   - -1 = no limit\n",
    "    min_data_in_leaf=1,\n",
    "    #   - 20 (default) is too big when data is small [https://github.com/Microsoft/LightGBM/issues/907]\n",
    "    # bagging_fraction=1.0,  # Default\n",
    "    bagging_fraction=0.632,  # Simulate data exposure from sampling with replacement, like rf\n",
    "    #   - Requires bagging_freq > 0\n",
    "    #   - Sample data per tree, without replacement; see these issues for with/without replacement discussion:\n",
    "    #       - https://github.com/Microsoft/LightGBM/pull/884\n",
    "    #       - https://github.com/Microsoft/LightGBM/issues/883\n",
    "    #       - https://github.com/Microsoft/LightGBM/issues/1038\n",
    "    # bagging_freq=0,  # Default\n",
    "    bagging_freq=1,\n",
    "    #   - Bag at every kth iteration (0 = disable)\n",
    "    #   - Requires bagging_fraction < 1\n",
    "    # feature_fraction=1.0,  # Default\n",
    "    feature_fraction=np.sqrt(f) / f,  # Like sk's default max_features='auto' behavior\n",
    "    #   - Default: 1.0\n",
    "    #   - Sample features per tree\n",
    "    # early_stopping_round=0,\n",
    "    #   - [Deprecated: moved to .train]\n",
    "    # max_delta_step=0.0,\n",
    "    #   - \"used to limit the max output of tree leaves\"\n",
    "    #   - \"<= 0 means no constraint\"\n",
    "    #   - \"the final max output of leaves is learning_rate * max_delta_step\"\n",
    "    # lambda_l1=0.0,\n",
    "    #   - L1 regularization\n",
    "    # lambda_l2=0.0,\n",
    "    #   - L2 regularization\n",
    "    # min_gain_to_split=0.0,\n",
    "    #   - \"the minimal gain to perform split\"\n",
    "\n",
    "    # IO Parameters\n",
    "    #   - TODO Grok more of these: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst#io-parameters\n",
    "    verbosity=1,\n",
    "    max_bin=63,\n",
    "    #   - Default: 255\n",
    "    #   - \"it is recommended to use the smaller max_bin (e.g. 63) to get the better speed up\" (from cpu/gpu)\n",
    "    #   - \"max number of bins that feature values will be bucketed in\"\n",
    "    #   - \"small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\"\n",
    "    #   - \"LightGBM will auto compress memory according to max_bin. For example, LightGBM will use uint8_t for\n",
    "    #     feature value if max_bin=255\"\n",
    "    # min_data_in_bin=3,\n",
    "    #   - \"use this to avoid one-data-one-bin (potential over-fitting)\"\n",
    "    # bin_construct_sample_cnt=200000,\n",
    "    #   - \"number of data that sampled to construct histogram bins\"\n",
    "    #   - \"setting this to larger value will give better training result, but will increase data loading time\"\n",
    "    #   - \"set this to larger value if data is very sparse\"\n",
    "    #   - TODO Tune with len(X)\n",
    "    # two_round=False,\n",
    "    #   - \"set this to true if data file is too big to fit in memory\"\n",
    "    # save_binary=False,\n",
    "    #   - \"save the dataset (including validation data) to a binary file. This speed ups the data loading for the next time\"\n",
    "    # enable_load_from_binary_file=True,\n",
    "    #   - \"set this to true to enable autoloading from previous saved binary datasets\"\n",
    "\n",
    "    # Objective Parameters\n",
    "    num_class=len(classes),\n",
    "    #   - Default: 1\n",
    "    #   - Required for objective='multiclass'\n",
    "\n",
    "    # Metric Parameters\n",
    "    #   - [ignore]\n",
    "\n",
    "    # Network Parameters\n",
    "    #   - [ignore]\n",
    "\n",
    "    # GPU Parameters\n",
    "    #   - [ignore]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "Check failed: num_tree_per_iteration_ == 1 at /Users/travis/miniconda3/conda-bld/lightgbm_1530780821674/work/compile/src/boosting/rf.hpp, line 41 .\n",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"<ipython-input-9-187013441d8c>\"\u001b[0m, line \u001b[1;32m11\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    keep_training_booster=True,\n",
      "  File \u001b[1;32m\"/Users/danb/miniconda3/envs/bubo-features/lib/python3.6/site-packages/lightgbm/engine.py\"\u001b[0m, line \u001b[1;32m183\u001b[0m, in \u001b[1;35mtrain\u001b[0m\n    booster = Booster(params=params, train_set=train_set)\n",
      "  File \u001b[1;32m\"/Users/danb/miniconda3/envs/bubo-features/lib/python3.6/site-packages/lightgbm/basic.py\"\u001b[0m, line \u001b[1;32m1309\u001b[0m, in \u001b[1;35m__init__\u001b[0m\n    ctypes.byref(self.handle)))\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/danb/miniconda3/envs/bubo-features/lib/python3.6/site-packages/lightgbm/basic.py\"\u001b[0;36m, line \u001b[0;32m49\u001b[0;36m, in \u001b[0;35m_safe_call\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\u001b[0m\n",
      "\u001b[0;31mLightGBMError\u001b[0m\u001b[0;31m:\u001b[0m Check failed: num_tree_per_iteration_ == 1 at /Users/travis/miniconda3/conda-bld/lightgbm_1530780821674/work/compile/src/boosting/rf.hpp, line 41 .\n\n"
     ]
    }
   ],
   "source": [
    "evals_result = {}\n",
    "gbm = lgb.train(\n",
    "    **lgb_train_kwargs,\n",
    "    params=lgb_params,\n",
    "    train_set=lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_test],\n",
    "    valid_names=['train', 'test'],\n",
    "    evals_result=evals_result,\n",
    "    verbose_eval=1,\n",
    "    #   - Print every n rounds\n",
    "    keep_training_booster=True,\n",
    "    #   - Default: False\n",
    "    #   - Whether to retain memory for further training [https://github.com/Microsoft/LightGBM/issues/668]\n",
    "    #   - FIXME .save_model only includes params if keep_training_booster=True\n",
    "    #       - Seems like it was intended to include params in both cases? [https://github.com/Microsoft/LightGBM/issues/1364]\n",
    ")\n",
    "display(\n",
    "    # evals_result,  # evals_result['test']['multi_logloss']: np.ndarray, same as the verbose output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Ah crap, rf mode doesn't allow multiclass:\n",
    "#   - https://github.com/Microsoft/LightGBM/blob/v2.1.2/src/boosting/rf.hpp#L40\n",
    "#   - https://github.com/Microsoft/LightGBM/issues/881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubo-features (PYTHONSTARTUP)",
   "language": "python",
   "name": "bubo-features (PYTHONSTARTUP)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
