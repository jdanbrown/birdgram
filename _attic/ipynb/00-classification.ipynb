{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "// $('#header-container').hide()        // Hide by default; toggle back on with cmd-shift-P -> \"toggle header\"\n// $('#maintoolbar').hide()             // Hide by default; toggle back on with cmd-shift-P -> \"toggle toolbar\"\n// $('.container').css('width', '100%') // Use full screen width",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    }
   ],
   "source": [
    "%%javascript\n",
    "// $('#header-container').hide()        // Hide by default; toggle back on with cmd-shift-P -> \"toggle header\"\n",
    "// $('#maintoolbar').hide()             // Hide by default; toggle back on with cmd-shift-P -> \"toggle toolbar\"\n",
    "// $('.container').css('width', '100%') // Use full screen width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize']      = (10, 10)  # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest' # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap']          = 'gray'    # use grayscale output rather than a (potentially misleading) color heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[\n",
    "    (0.15045533, 'n04525038 velvet'),\n",
    "    (0.13852249, 'n03223299 doormat, welcome mat'),\n",
    "    (0.12479795, 'n03207743 dishrag, dishcloth'),\n",
    "    (0.08279831, 'n04599235 wool, woolen, woollen'),\n",
    "    (0.033983503, 'n03887697 paper towel'),\n",
    "    (0.022745488, 'n03530642 honeycomb'),\n",
    "    (0.022661999, 'n04376876 syringe'),\n",
    "    (0.016129013, 'n03000247 chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour'),\n",
    "    (0.011925566, 'n03998194 prayer rug, prayer mat'),\n",
    "    (0.011500423, 'n02114855 coyote, prairie wolf, brush wolf, Canis latrans'),\n",
    "    (0.01124437, 'n04332243 strainer'),\n",
    "    (0.011015759, 'n02454379 armadillo'),\n",
    "    (0.0097646723, 'n02120505 grey fox, gray fox, Urocyon cinereoargenteus'),\n",
    "    (0.0097130509, 'n04589890 window screen'),\n",
    "    (0.0090373242, 'n02219486 ant, emmet, pismire'),\n",
    "    (0.0090282969, 'n02786058 Band Aid'),\n",
    "    (0.0074922051, 'n03000134 chainlink fence'),\n",
    "    (0.0073114005, 'n01756291 sidewinder, horned rattlesnake, Crotalus cerastes'),\n",
    "    (0.0071736509, 'n03717622 manhole cover'),\n",
    "    (0.0070126946, 'n04372370 switch, electric switch, electrical switch'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "caffe_root = '../caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "# If you get \"No module named _caffe\", either you have not built pycaffe or you have the wrong path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_id = 'bvlc_reference_caffenet' # A variant of alexnet\n",
    "#model_id = 'bvlc_googlenet' # TODO\n",
    "#model_id = 'bvlc_alexnet'\n",
    "\n",
    "model_def     = '%(caffe_root)s/models/%(model_id)s/deploy.prototxt'         % locals()\n",
    "model_weights = '%(caffe_root)s/models/%(model_id)s/%(model_id)s.caffemodel' % locals()\n",
    "\n",
    "if os.path.isfile(model_weights):\n",
    "    print 'Found model[%(model_id)s]' % locals()\n",
    "else:\n",
    "    print 'Downloading model[%(model_id)s]...' % locals()\n",
    "    !../scripts/download_model_binary.py ../models/{model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_cpu()\n",
    "\n",
    "net = caffe.Net(\n",
    "    model_def,      # defines the structure of the model\n",
    "    model_weights,  # contains the trained weights\n",
    "    caffe.TEST,     # use test mode (e.g., don't perform dropout)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set up input preprocessing. (We'll use Caffe's `caffe.io.Transformer` to do this, but this step is independent of other parts of Caffe, so any custom preprocessing code may be used).\n",
    "\n",
    "    Our default CaffeNet is configured to take images in BGR format. Values are expected to start in the range [0, 255] and then have the mean ImageNet pixel value subtracted from them. In addition, the channel dimension is expected as the first (_outermost_) dimension.\n",
    "\n",
    "    As matplotlib will load images with values in the range [0, 1] in RGB format with the channel as the _innermost_ dimension, we are arranging for the needed transformations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the mean ImageNet image (as distributed with Caffe) for subtraction\n",
    "mu = np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
    "mu = mu.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "print 'mean-subtracted values:', zip('BGR', mu)\n",
    "\n",
    "# create transformer for the input called 'data'\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "\n",
    "transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we're ready to perform classification. Even though we'll only classify one image, we'll set a batch size of 50 to demonstrate batching.\n",
    "\n",
    "- Since we're dealing with four-dimensional data here, we'll define a helper function for visualizing sets of rectangular heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the size of the input (we can skip this if we're happy\n",
    "# with the default; we can also change it later, e.g., for different batch sizes)\n",
    "net.blobs['data'].reshape(\n",
    "    50,        # batch size\n",
    "    3,         # 3-channel (BGR) images\n",
    "    227, 227,  # image size is 227x227\n",
    "    #224, 224,  # image size is 227x227 [TODO Make googlenet work]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pick your image\n",
    "#image_path = caffe_root + 'examples/images/cat.jpg'\n",
    "image_path = '../data/MLSP 2013/mlsp_contest_dataset/supplemental_data/spectrograms/PC1_20090705_070000_0040.bmp'\n",
    "#image_path = '../data/recordings/chickadee funny noise, other chirp song sparrow.wav'\n",
    "\n",
    "image = caffe.io.load_image(image_path)\n",
    "transformed_image = transformer.preprocess('data', image)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy the image data into the memory allocated for the net\n",
    "net.blobs['data'].data[...] = transformed_image\n",
    "\n",
    "# perform classification (slow, ~secs)\n",
    "output = net.forward()\n",
    "\n",
    "output_prob = output['prob'][0] # the output probability vector for the first image in the batch\n",
    "\n",
    "print 'predicted class is:', output_prob.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The net gives us a vector of probabilities; the most probable class was the 281st one. But is that correct? Let's check the ImageNet labels...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load ImageNet labels\n",
    "labels_file = caffe_root + 'data/ilsvrc12/synset_words.txt'\n",
    "if not os.path.exists(labels_file):\n",
    "    !../data/ilsvrc12/get_ilsvrc_aux.sh\n",
    "\n",
    "labels = np.loadtxt(labels_file, str, delimiter='\\t')\n",
    "\n",
    "print 'probabilities and labels:'\n",
    "top_inds = output_prob.argsort()[::-1][:20] # top k predictions from softmax output\n",
    "pprint(zip(output_prob[top_inds], labels[top_inds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topologies of common networks:\n",
    "- http://ethereon.github.io/netscope/#/preset/alexnet\n",
    "- http://ethereon.github.io/netscope/#/preset/caffenet\n",
    "- http://ethereon.github.io/netscope/#/preset/googlenet\n",
    "- http://ethereon.github.io/netscope/#/preset/vgg-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each layer, show the shapes of the activations and params:\n",
    "#   blob       activations  (batch_size, channel_dim, height, width) -- typically, but not always\n",
    "#   params[0]  weights      (output_channels, input_channels, filter_height, filter_width)\n",
    "#   params[1]  biases       (output_channels,)\n",
    "\n",
    "def show_shape(shape, name, fields):\n",
    "    return '%s(%s)' % (name, ', '.join(['%s=%s' % (d,s) for (s,d) in zip(shape, fields)]))\n",
    "\n",
    "for layer_name, blob in net.blobs.iteritems():\n",
    "    [param_weights, param_biases] = net.params.get(layer_name, [None, None])\n",
    "    print '%-52s %-29s %-31s %s' % (\n",
    "        layer_name,\n",
    "        show_shape(blob.data.shape, 'act', ('b', 'c', 'h', 'w')),\n",
    "        param_weights and show_shape(param_weights.data.shape, 'weight', ('o', 'i', 'h', 'w')) or '',\n",
    "        param_biases  and show_shape(param_biases.data.shape,  'bias',   ('o', 'i', 'h', 'w')) or '',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm(data):\n",
    "    return (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "def tile(data, w = np.sqrt):\n",
    "\n",
    "    n = data.shape[0]\n",
    "    w = int(np.ceil(w(n)))\n",
    "    h = int(np.ceil(n / float(w)))\n",
    "    padding = (\n",
    "        ((0, h*w-n), (0, 1), (0, 1))  # add some space between filters\n",
    "        + ((0, 0),) * (data.ndim - 3) # don't pad the last dimension (if there is one)\n",
    "    )\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=1)# pad with ones (white)\n",
    "\n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((h,w) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((h * data.shape[1], w * data.shape[3]) + data.shape[4:])\n",
    "    return data\n",
    "\n",
    "def vis_pretiled(data):\n",
    "    plt.figure(figsize = (29, 29*6)) # scale to fit within 17in width x 10in height\n",
    "    print 'vis_pretiled: data.shape[%s]' % (data.shape,) # XXX\n",
    "    plt.imshow(data)\n",
    "    plt.axis('off')\n",
    "    plt.show() # So we can have multiple plt.imshow's from the same cell\n",
    "\n",
    "def vis_square(data, w = np.sqrt):\n",
    "    'data: an array of shape (n, height, width) or (n, height, width, 3)'\n",
    "    vis_pretiled(tile(norm(data), w))\n",
    "\n",
    "def TODO_tiled(data, w = np.sqrt):\n",
    "    return tile(norm(data), w)\n",
    "\n",
    "def tile_tiles(data, w_filters_per_tile = np.sqrt):\n",
    "    print 'tile_tile: data.shape[%s]' % (data.shape,)\n",
    "\n",
    "    # Calculate our various widths (w_*)\n",
    "    w_pixels_per_figure = 400\n",
    "    (n_out, n_in, h_pixels_per_filter, w_pixels_per_filter) = data.shape\n",
    "    w_filters_per_tile = int(np.ceil(w_filters_per_tile(n_in)))\n",
    "    w_tiles_per_figure = w_pixels_per_figure / w_filters_per_tile / w_pixels_per_filter\n",
    "\n",
    "    return tile(\n",
    "        w    = lambda n: w_tiles_per_figure,\n",
    "        data = np.array(map(\n",
    "            w    = lambda xs: tile(lambda n: w_filters_per_tile, xs),\n",
    "            data = np.array(map(norm, data)),\n",
    "        )),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_pretiled(TODO_tiled(net.params['conv1'][0].data.transpose(0,2,3,1), lambda n: 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vis_pretiled(tile_tiles(net.params['conv2'][0].data))\n",
    "#vis_pretiled(tile_tiles(net.params['conv3'][0].data))\n",
    "#vis_pretiled(tile_tiles(net.params['conv4'][0].data))\n",
    "#vis_pretiled(tile_tiles(net.params['conv5'][0].data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO Backup before modifying above\n",
    "#\n",
    "#def norm(data):\n",
    "#    return (data - data.min()) / (data.max() - data.min())\n",
    "#\n",
    "#def tile(aspect, data):\n",
    "#    orig_shape = data.shape\n",
    "#\n",
    "#    # Decompose n = w*h where w/h = aspect\n",
    "#    n = data.shape[0]\n",
    "#    w  = int(np.ceil(np.sqrt(n / float(aspect))))\n",
    "#    #h = int(np.ceil(np.sqrt(n * float(aspect)))) # Correct approximation to h/w = aspect\n",
    "#    h  = int(np.ceil(n / float(w)))               # Smaller h that avoids O(w) blank space at the bottom\n",
    "#    padding = (\n",
    "#        ((0, w*h-n), (0, 1), (0, 1))  # add some space between filters\n",
    "#        + ((0, 0),) * (data.ndim - 3) # don't pad the last dimension (if there is one)\n",
    "#    )\n",
    "#    data = np.pad(data, padding, mode='constant', constant_values=1)  # pad with ones (white)\n",
    "#\n",
    "#    # tile the filters into an image\n",
    "#    data = data.reshape((h,w) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "#    data = data.reshape((h * data.shape[1], w * data.shape[3]) + data.shape[4:])\n",
    "#\n",
    "#    print 'tile_data shape: %s -> %s' % (orig_shape, data.shape)\n",
    "#    return data\n",
    "#\n",
    "#def vis_pretiled(data):\n",
    "#    plt.figure(figsize = (17, 100)) # scale to fit within 17in width x 10in height\n",
    "#    print 'vis_pretiled: data.shape[%s]' % (data.shape,) # XXX\n",
    "#    plt.imshow(data)\n",
    "#    plt.axis('off')\n",
    "#    plt.show() # So we can have multiple plt.imshow's from the same cell\n",
    "#\n",
    "#def vis_square(data):\n",
    "#    'data: an array of shape (n, height, width) or (n, height, width, 3)'\n",
    "#    vis_pretiled(tile(1, norm(data)))\n",
    "#\n",
    "#print net.params['conv2'][0].data.shape\n",
    "#print net.params['conv2'][0].data.transpose(0,1,2,3).reshape(256*48,5,5).shape\n",
    "#\n",
    "#vis_pretiled(tile(1/6.0, tile(1, norm(net.params['conv2'][0].data.transpose(1,2,3,0))).transpose(2,0,1)))\n",
    "#vis_square(tile(1, net.params['conv2'][0].data.transpose(1,2,3,0)).transpose(2,0,1))\n",
    "##vis_square(net.params['conv2'][0].data.transpose(0,1,2,3).reshape(256*48,5,5))\n",
    "#vis_square(net.params['conv2'][0].data.transpose(0,1,2,3)[0])\n",
    "#vis_square(net.params['conv2'][0].data.transpose(1,0,2,3)[0])\n",
    "#\n",
    "#vis_square(net.params['conv1'][0].data.transpose(0,2,3,1)) # (96,3,11,11) -> (96,11,11,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# XXX Merge into below after making this work\n",
    "\n",
    "for layer_name, params in net.params.iteritems():\n",
    "    [param_weights, param_biases] = net.params.get(layer_name, [None, None])\n",
    "    if param_weights:\n",
    "        print 'layer[%s], param_weights.data.shape[%s]' % (layer_name, param_weights.data.shape)\n",
    "        try:\n",
    "            vis_square(param_weights.data.transpose(0, 2, 3, 1)) # e.g. (96,3,11,11) -> (96,11,11,3)\n",
    "        except Exception, e:\n",
    "            print '    Error:', e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer_name, blob in net.blobs.iteritems():\n",
    "\n",
    "    [param_weights, param_biases] = net.params.get(layer_name, [None, None])\n",
    "    print '\\nlayer[%s]\\n- blob.data.shape[%s]\\n- param_weights.data.shape[%s]\\n- param_biases.data.shape[%s]\\n' % (\n",
    "        layer_name,\n",
    "        blob.data.shape,\n",
    "        param_weights and param_weights.data.shape,\n",
    "        param_biases  and param_biases.data.shape,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        vis_square(blob.data[0])\n",
    "    except Exception, e:\n",
    "        print '    Error:', e\n",
    "\n",
    "    if not param_weights:\n",
    "        print 'layer[%s], param_weights[%s]' % (layer_name, param_weights)\n",
    "    else:\n",
    "        print 'layer[%s], param_weights.data.shape[%s]' % (layer_name, param_weights.data.shape)\n",
    "        try:\n",
    "            vis_square(param_weights.data.transpose(0, 2, 3, 1)) # e.g. (96,3,11,11) -> (96,11,11,3)\n",
    "        except Exception, e:\n",
    "            print '    Error:', e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'probabilities and labels:'\n",
    "top_inds = output_prob.argsort()[::-1][:20] # top k predictions from softmax output\n",
    "pprint(zip(output_prob[top_inds], labels[top_inds]))\n",
    "\n",
    "# The first fully connected layer, `fc6` (rectified)\n",
    "# - We show the output values and the histogram of the positive values\n",
    "plt.subplot(3, 1, 1); plt.plot(net.blobs['fc6'].data[0].flat)\n",
    "plt.subplot(3, 1, 2); plt.hist(net.blobs['fc6'].data[0].flat[net.blobs['fc6'].data[0].flat > 0], bins=100)\n",
    "\n",
    "# The final probability output, `prob`\n",
    "# - Note the cluster of strong predictions; the labels are sorted semantically\n",
    "# - The top peaks correspond to the top predicted labels, as shown above\n",
    "plt.subplot(3, 1, 3); plt.plot(net.blobs['prob'].data[0].flat)"
   ]
  }
 ],
 "metadata": {
  "description": "Instant recognition with a pre-trained model and a tour of the net interface for visualizing features and parameters layer-by-layer.",
  "example_name": "Image Classification and Filter Visualization",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "_global (PYTHONSTARTUP)",
   "language": "python",
   "name": "_global (PYTHONSTARTUP)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "priority": 1
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
