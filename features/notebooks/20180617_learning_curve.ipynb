{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.267s"
   },
   "outputs": [],
   "source": [
    "from notebooks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = Load()\n",
    "# with cache_control():  # FIXME Hangs without refresh=True [why?] -- which fortunately doesn't replay the slow .mp3->.wav step\n",
    "# with cache_control(refresh=True):\n",
    "with cache_control(refresh=False):\n",
    "    # recs_all = recs_all if 'recs_all' in locals() else load.recs(\n",
    "    recs_all = load.recs(\n",
    "        # limit=150,  # XXX Faster dev (need >100 to get >1 species, else things below break)\n",
    "        datasets=[\n",
    "            # 'peterson-field-guide',\n",
    "            # 'recordings',\n",
    "            'xc',\n",
    "        ],\n",
    "    )\n",
    "display(\n",
    "    df_summary(recs_all).T,\n",
    "    # df_summary(recs_all),\n",
    "    recs_all[:5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "(recs_all\n",
    "    .assign(species=lambda df: df.species_longhand)\n",
    "    .assign(count=1)\n",
    "    # 0-fill all (species, dataset) combinations to create empty placeholders for missing bars\n",
    "    .pipe(lambda df: df.append(\n",
    "        pd.DataFrame([\n",
    "            dict(species=species, dataset=dataset, duration_s=0)\n",
    "            for species in df.species.unique()\n",
    "            for dataset in df.dataset.unique()\n",
    "        ])\n",
    "        .astype({'species': df.species.dtype})\n",
    "    ))\n",
    "    .groupby(['dataset', 'species'])[['count', 'duration_s']].sum().reset_index()\n",
    "    .pipe(df_ordered_cat, species=lambda df: df.sort_values('count').species)  # Order species by count\n",
    "    .pipe(pd.melt, id_vars=['dataset', 'species'], value_vars=['count', 'duration_s'])\n",
    "    # .pipe(df_reverse_cat, 'species')  # Order species by taxo\n",
    "    .pipe(ggplot, aes(x='species', y='value', fill='dataset', color='dataset'))\n",
    "    + coord_flip()\n",
    "    + geom_bar(stat='identity', position=position_dodge(), width=.8)\n",
    "    + facet_wrap('variable', nrow=1, scales='free')\n",
    "    + xlab('')\n",
    "    + ylab('')\n",
    "    + scale_fill_cmap_d(mpl.cm.tab10)\n",
    "    + scale_color_cmap_d(mpl.cm.tab10)\n",
    "    + theme(panel_spacing=2.5)\n",
    "    + theme_figsize(width=18, aspect_ratio=4/1)\n",
    "    + ggtitle(f'recs_all: Total (count, duration_s) per (species, dataset)')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.766s"
   },
   "outputs": [],
   "source": [
    "# Subset the data\n",
    "#   - Runtime: reduce data volume\n",
    "#   - Class imbalance: clip all classes at the same N (or smaller)\n",
    "class_count = np.inf   # All species (168)\n",
    "# class_count = 10     # Faster dev\n",
    "# class_count = 2      # Faster dev\n",
    "# class_size = np.inf  # recs[14k]  @ species[168]\n",
    "# class_size = 200     # recs[12k]  @ species[168]\n",
    "class_size = 100     # recs[10k]  @ species[168]\n",
    "# class_size = 50      # recs[7.0k] @ species[168]\n",
    "# class_size = 20      # recs[3.2k] @ species[168]\n",
    "# class_size = 10      # recs[1.7k] @ species[168]\n",
    "# class_size = 5       # recs[.84k] @ species[168]\n",
    "# class_size = 2       # recs[.34k] @ species[168]\n",
    "recs = (recs_all\n",
    "    # Sample class_size per species\n",
    "    .groupby('species').apply(lambda g: g.sample(n=min(len(g), class_size), random_state=0))\n",
    "    # Drop classes with <2 instances, else StratifiedShuffleSplit complains (e.g. 'TUVU')\n",
    "    [lambda df: df.species.isin(df.species.value_counts()[lambda s: s >= 2].index)]\n",
    "    # Sample class_count of the species\n",
    "    [lambda df: df.species.isin(df.species.sample(n=min(len(df.species), class_count), random_state=0))]\n",
    ")\n",
    "display(\n",
    "    df_summary(recs_all).T,\n",
    "    df_summary(recs).T,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": false,
    "time": "7s"
   },
   "outputs": [],
   "source": [
    "(recs\n",
    "    .assign(species=lambda df: df.species_longhand)\n",
    "    .assign(count=1)\n",
    "    # 0-fill all (species, dataset) combinations to create empty placeholders for missing bars\n",
    "    .pipe(lambda df: df.append(\n",
    "        pd.DataFrame([\n",
    "            dict(species=species, dataset=dataset, duration_s=0)\n",
    "            for species in df.species.unique()\n",
    "            for dataset in df.dataset.unique()\n",
    "        ])\n",
    "        .astype({'species': df.species.dtype})\n",
    "    ))\n",
    "    .groupby(['dataset', 'species'])[['count', 'duration_s']].sum().reset_index()\n",
    "    .pipe(df_ordered_cat, species=lambda df: df.sort_values('count').species)  # Order species by count\n",
    "    .pipe(pd.melt, id_vars=['dataset', 'species'], value_vars=['count', 'duration_s'])\n",
    "    # .pipe(df_reverse_cat, 'species')  # Order species by taxo\n",
    "    .pipe(df_remove_unused_categories)\n",
    "    .pipe(ggplot, aes(x='species', y='value', fill='dataset', color='dataset'))\n",
    "    + coord_flip()\n",
    "    + geom_bar(stat='identity', position=position_dodge(), width=.8)\n",
    "    + facet_wrap('variable', nrow=1, scales='free')\n",
    "    + xlab('')\n",
    "    + ylab('')\n",
    "    + scale_fill_cmap_d(mpl.cm.tab10)\n",
    "    + scale_color_cmap_d(mpl.cm.tab10)\n",
    "    + theme(panel_spacing=2.5)\n",
    "    + theme_figsize(width=18, aspect_ratio=4/1)\n",
    "    + ggtitle(f'recs: Total (count, duration_s) per (species, dataset)')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved projection model\n",
    "projection = Projection.load('peterson-v0-26bae1c', features=Features(load=load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "2.422s"
   },
   "outputs": [],
   "source": [
    "# Add .feat\n",
    "recs = projection.transform(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "5.949s"
   },
   "outputs": [],
   "source": [
    "# GridSearchCV many models / model params\n",
    "param_grid = dict(\n",
    "    classifier=[\n",
    "\n",
    "        # # KNN\n",
    "        # #   - Bad for large k because we only have ~7-26 instances per class\n",
    "        # #   - Bad for small k because KNN\n",
    "        # # 'cls: knn, n_neighbors: 100',\n",
    "        # 'cls: knn, n_neighbors: 50',\n",
    "        # 'cls: knn, n_neighbors: 25',\n",
    "        # # 'cls: knn, n_neighbors: 15',\n",
    "        # 'cls: knn, n_neighbors: 10',\n",
    "        # 'cls: knn, n_neighbors: 5',\n",
    "        # 'cls: knn, n_neighbors: 3',\n",
    "        # 'cls: knn, n_neighbors: 1',\n",
    "\n",
    "        # # SVM(RBF)\n",
    "        # #   - [SBF16] Uses these\n",
    "        # 'cls: svm, random_state: 0, probability: true, kernel: rbf, C: 1',\n",
    "        # 'cls: svm, random_state: 0, probability: true, kernel: rbf, C: 10',  # [SBF16]\n",
    "        # 'cls: svm, random_state: 0, probability: true, kernel: rbf, C: 100',  # Same as C:10 [why?]\n",
    "        # # 'cls: svm, random_state: 0, probability: true, kernel: rbf, C: 1000',  # Same as C:10 [why?]\n",
    "\n",
    "        # # RandomForest(entropy)\n",
    "        # #   - [SP14] uses entropy\n",
    "        # # 'cls: rf, random_state: 0, criterion: entropy, n_estimators: 10',\n",
    "        # 'cls: rf, random_state: 0, criterion: entropy, n_estimators: 50',\n",
    "        # 'cls: rf, random_state: 0, criterion: entropy, n_estimators: 100',\n",
    "        # 'cls: rf, random_state: 0, criterion: entropy, n_estimators: 200',  # [SP14]\n",
    "        # 'cls: rf, random_state: 0, criterion: entropy, n_estimators: 400',\n",
    "        # 'cls: rf, random_state: 0, criterion: entropy, n_estimators: 800',\n",
    "        # 'cls: rf, random_state: 0, criterion: entropy, n_estimators: 1000',\n",
    "\n",
    "        # # RandomForest(gini)\n",
    "        # #   - sk uses gini by default\n",
    "        # 'cls: rf, random_state: 0, criterion: gini, n_estimators: 1',  # Faster dev\n",
    "        # 'cls: rf, random_state: 0, criterion: gini, n_estimators: 10',  # [sk default]\n",
    "        # 'cls: rf, random_state: 0, criterion: gini, n_estimators: 50',\n",
    "        # 'cls: rf, random_state: 0, criterion: gini, n_estimators: 100',\n",
    "        # 'cls: rf, random_state: 0, criterion: gini, n_estimators: 200',\n",
    "        # 'cls: rf, random_state: 0, criterion: gini, n_estimators: 400',\n",
    "        # 'cls: rf, random_state: 0, criterion: gini, n_estimators: 600',\n",
    "        # 'cls: rf, random_state: 0, criterion: gini, n_estimators: 800',\n",
    "        'cls: rf, random_state: 0, criterion: gini, n_estimators: 1000',\n",
    "        # WARNING These two are not very mem safe...\n",
    "        #   - They finish, but they cause OSX to \"Not responding\" most/all of its apps at ~2-3 different times\n",
    "        # 'cls: rf, random_state: 0, criterion: gini, n_estimators: 1500',\n",
    "        # 'cls: rf, random_state: 0, criterion: gini, n_estimators: 2000',\n",
    "\n",
    "    ],\n",
    "    # Downsample for learning curves\n",
    "    downsample_classes=[\n",
    "        # .1, # Faster dev\n",
    "        # .1, 1.0,  # Faster dev\n",
    "        100, 75, 50, 25,  # Biggest first, to fail fast\n",
    "    ],\n",
    "    # TODO agg_funs\n",
    ")\n",
    "\n",
    "# TODO Increase cv to decrease variance in eval metrics (this made Alex extremely squirmy)\n",
    "cv = GridSearchCVCached(\n",
    "    estimator=Search(projection=projection),\n",
    "    param_grid=param_grid,\n",
    "    refit=False,  # Don't spend time fitting cv.best_estimator_ at the end (default: True)\n",
    "    # cv=3,  # [SP14] uses two-/three-fold CV [why?]\n",
    "    # Stratified ensures that all classes have >0 instances in each split, which is statistically maybe fishy but avoids\n",
    "    # breaking various code that merges the split results back together and assumes all splits' classes are the same\n",
    "    cv=sk.model_selection.StratifiedShuffleSplit(\n",
    "        # n_splits=1,  # [for ~18/20 miss: ~19m, ~16g disk cache]\n",
    "        # n_splits=3,\n",
    "        # n_splits=5,\n",
    "        # n_splits=10,\n",
    "        n_splits=20,  # Known good [>51m uncached, >25g disk cache]\n",
    "        # n_splits=100,  # [?m runtime, ?g disk cache]\n",
    "        test_size=.2,\n",
    "        random_state=0,\n",
    "    ),\n",
    "    return_train_score=True,\n",
    "    extra_metrics=dict(\n",
    "        # [How to specify SearchEvals here without creating caching headaches?]\n",
    "        #   - e.g. defs don't bust cache on code edit\n",
    "        #   - And avoid thrashing cache every time we refactor SearchEvals\n",
    "        test_i='i_test',\n",
    "        test_y='y_test',\n",
    "        test_classes='estimator.classes_',\n",
    "        test_predict_proba='estimator.classifier_.predict_proba(X_test)',\n",
    "    ),\n",
    "    # return_estimator=True,  # Verrrry heavy, use extra_metrics instead\n",
    "    # verbose=100,\n",
    "    # verbose=10,  # O(models * n_splits) lines of outputs\n",
    "    verbose=1,  # O(1) lines of outputs\n",
    "    # n_jobs=1,  # For %prun\n",
    "    # n_jobs=6,  # Doesn't peg all 8 hyperthreads\n",
    "    n_jobs=8,\n",
    "    # n_jobs=16,  # Not mem safe, in general (laptop OOMs with 16x rf classifiers)\n",
    ")\n",
    "with contextlib.ExitStack() as stack:\n",
    "    # stack.enter_context(cache_control(refresh=True))  # Disk unsafe...\n",
    "    stack.enter_context(cache_control(enabled=False))  # Disk safe\n",
    "    # stack.enter_context(joblib.parallel_backend('threading'))  # Default: 'multiprocessing'\n",
    "    # stack.enter_context(joblib.parallel_backend('sequential'))  # For %prun [FIXME Has no effect; why?]\n",
    "    stack.enter_context(log.context(level='info'))\n",
    "    X, y = Search.Xy(recs)\n",
    "    cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute eval metrics + dims for the plots below\n",
    "#   - Grain: models\n",
    "#   - Metrics: train_score, test_score, ...\n",
    "#   - Dims: model_id, classifier_type, fold, ...\n",
    "cv_models = (cv_results_splits_df(cv.cv_results_)\n",
    "    # Slow yaml parsing, compute up front\n",
    "    .assign(params_dict=lambda df: df.apply(axis=1, func=lambda row: {\n",
    "        **{strip_startswith(k, 'param_'): row[k] for k in df if k.startswith('param_')},\n",
    "        'classifier': yaml.safe_load('{%s}' % row.param_classifier),\n",
    "    }))\n",
    "    [lambda df: [c for c in df if not c.startswith('param_')]]\n",
    "    # Dims from params\n",
    "    .assign(\n",
    "        # Useful for validation curves (vary params by classifier type)\n",
    "        # classifier_type=lambda df: df.params_dict.str['classifier'].apply(lambda x: (\n",
    "        #     '%s-%s' % (x['cls'], x['criterion']) if x['cls'] == 'rf' else\n",
    "        #     x['cls']\n",
    "        # )),\n",
    "        # Useful for learning curves (vary downsample_classes per classifier)\n",
    "        classifier_type=lambda df: df.params_dict.apply(lambda d: ', '.join(\n",
    "            '%s[%s]' % (k, v) for k, v in d.items() if k not in ['downsample_classes']\n",
    "        )),\n",
    "    )\n",
    "    # .eval\n",
    "    .assign(test_evals=lambda df: np.vectorize(SearchEvals)(\n",
    "        i=df.pop('test_i'),\n",
    "        y=df.pop('test_y'),\n",
    "        classes=df.pop('test_classes'),\n",
    "        y_scores=df.pop('test_predict_proba'),\n",
    "    ))\n",
    "    # Reorder\n",
    "    .pipe(df_reorder_cols, first=[\n",
    "        'model_id', 'params', 'params_dict', 'classifier_type',\n",
    "    ])\n",
    ")\n",
    "param_list = list(cv_models.params.unique())\n",
    "display(\n",
    "    df_summary(cv_models).T,\n",
    "    # cv_models[:5],\n",
    "    cv_models,\n",
    "    param_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.467s"
   },
   "outputs": [],
   "source": [
    "# Train/test scores\n",
    "#   - Grain: models x {train,test}\n",
    "#   - Metrics: score\n",
    "#   - Dims: group, params\n",
    "(cv_models\n",
    "    .pipe(lambda df: pd.melt(\n",
    "        df,\n",
    "        id_vars=['classifier_type', 'params', 'fold'],\n",
    "        value_vars=['train_score', 'test_score'],\n",
    "        var_name='split',\n",
    "        value_name='score'),\n",
    "    )\n",
    "    .assign(\n",
    "        split=lambda df: df.split.str.replace('_score', ''),\n",
    "        group=lambda df: df.apply(axis=1, func=lambda row: row.classifier_type + '/' + row.split),\n",
    "    )\n",
    "    .pipe(df_ordered_cat, params=lambda df: param_list)\n",
    "    .pipe(ggplot, aes(x='params', y='score', color='group'))\n",
    "    + geom_hline(yintercept=0, color='grey')\n",
    "    + geom_jitter(alpha=.5, width=.05, height=1e-9)\n",
    "    # + geom_point(alpha=.5)\n",
    "    + geom_line(aes(group='group + str(fold)'), alpha=.2)\n",
    "    + stat_summary(aes(group='group'), fun_data='mean_cl_boot', random_state=0, geom='errorbar')\n",
    "    + stat_summary(aes(group='group'), fun_data='mean_cl_boot', random_state=0, geom='line', size=1)\n",
    "    + scale_color_cmap_d(mpl.cm.tab10)\n",
    "    + theme(axis_text_x=element_text(angle=90, hjust=.5))\n",
    "    + theme_figsize(width=12, aspect_ratio=1/2)\n",
    "    # + coord_cartesian(ylim=(-35, 0))\n",
    "    + ylab('score (-coverage_error)')\n",
    "    + ggtitle('Train/test scores')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model diagnostics: all models, all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('models:')\n",
    "print('  params[*/%s]' % len(param_list))\n",
    "print('  fold[*/%s]' % cv.cv.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "69.296s"
   },
   "outputs": [],
   "source": [
    "# TODO Cache\n",
    "# Coverage errors: all models, all folds\n",
    "#   - Subset: all models\n",
    "#   - Grain: sum(recs[model.test_i] for model)\n",
    "#   - Dims: model_id, params, fold, y_true, rec_id\n",
    "#   - Metrics: coverage_error\n",
    "coverage_errors_all_all = (cv_models\n",
    "    # .sample(n=5, random_state=0)  # For faster dev\n",
    "    .pipe(lambda df: DF(\n",
    "        OrderedDict(\n",
    "            # **row[['model_id', 'params', 'fold']],  # Slow (in this inner loop), unpack manually instead\n",
    "            model_id=row.model_id,\n",
    "            params=row.params,\n",
    "            classifier_type=row.classifier_type,\n",
    "            fold=row.fold,\n",
    "            i=i,\n",
    "            y_true=y_true,\n",
    "            coverage_error=coverage_error,\n",
    "        )\n",
    "        for row in iter_progress(df_rows(df), n=len(df))\n",
    "        for i, y_true, coverage_error in zip(\n",
    "            row.test_evals.i,\n",
    "            row.test_evals.y,\n",
    "            row.test_evals.coverage_errors(),\n",
    "        )\n",
    "    ))\n",
    ")\n",
    "display(\n",
    "    df_summary(coverage_errors_all_all).T,\n",
    "    coverage_errors_all_all[:10],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "8.431s"
   },
   "outputs": [],
   "source": [
    "# Facet by classifier_type, order by median(coverage_error)\n",
    "#   - Subset: all models\n",
    "#   - Grain: sum(recs[model.test_i].groupby(params, y_true) for model)\n",
    "#       - Over: fold, rec_id\n",
    "#   - Dims: params, y_true\n",
    "#   - Metrics: coverage_error.median\n",
    "# in: coverage_errors_all_all, recs\n",
    "(coverage_errors_all_all\n",
    "    # Sort species by median(coverage_error) (across all models)\n",
    "    .pipe(df_ordered_cat, y_true=lambda df: (\n",
    "        df.groupby('y_true').agg({'coverage_error': np.median}).reset_index().sort_values('coverage_error').y_true\n",
    "    ))\n",
    "    .pipe(ggplot, aes(x='y_true', y='coverage_error', color='params'))\n",
    "    + facet_wrap('classifier_type')\n",
    "    + geom_line(aes(group='params'), stat='summary', fun_y=np.median)\n",
    "    + coord_flip(ylim=(0, min(30, len(np.unique(y)))))\n",
    "    + geom_hline(yintercept=recs.species.nunique(), color='grey')\n",
    "    + scale_color_cmap_d(mpl.cm.tab20)\n",
    "    + theme_figsize('square')\n",
    "    # + theme_figsize('full')\n",
    "    # + theme_figsize('full_dense')\n",
    "    + ggtitle(rf'Coverage error over fold$\\times$instance, by params')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "126.168s"
   },
   "outputs": [],
   "source": [
    "# TODO Slow, bad for notebook dev loop -- move lower, or disable by default?\n",
    "# Facet by species\n",
    "#   - Subset: all models\n",
    "#   - Grain: sum(recs[model.test_i].groupby(params, y_true) for model)\n",
    "#       - Over: fold, rec_id\n",
    "#   - Dims: params, y_true\n",
    "#   - Metrics: coverage_error.median\n",
    "# in: coverage_errors_all_all, recs\n",
    "(coverage_errors_all_all\n",
    "    # .sample(200, random_state=0)  # Faster dev\n",
    "    .astype({'y_true': metadata.species.df.shorthand.dtype})\n",
    "    .pipe(df_ordered_cat, params=lambda df: reversed(param_list))\n",
    "    .pipe(ggplot, aes(x='params', y='coverage_error', color='classifier_type'))\n",
    "    + facet_wrap('y_true')\n",
    "    + geom_hline(yintercept=1, color='grey')\n",
    "    + geom_hline(yintercept=recs.species.nunique(), color='grey')\n",
    "\n",
    "    # Percentiles (faster, no overplot)\n",
    "    # + geom_point(stat='summary', fun_y=np.median)\n",
    "    # + geom_linerange(stat='summary', fun_ymin=partial(np.percentile, q=25), fun_ymax=partial(np.percentile, q=75))\n",
    "\n",
    "    # Violin (slow, no overplot)\n",
    "    # + geom_violin()\n",
    "\n",
    "    # Boxplot (very slow, no overplot)\n",
    "    # + geom_boxplot()\n",
    "\n",
    "    # Points (medium cost, high overplot)\n",
    "    #   - n (count) instead of prop (proportion)\n",
    "    #   - scale_size_area() instead of default scale_size(), because it's a count [I don't grok this but it looks good]\n",
    "    + geom_count(aes(size='..n..'), alpha=.5)\n",
    "    + scale_size_area()\n",
    "    + geom_point(stat='summary', fun_y=np.median, alpha=1, color='black', shape='|', size=3, stroke=1)\n",
    "\n",
    "    + coord_flip()\n",
    "    + scale_color_cmap_d(mpl.cm.tab20)\n",
    "    + theme(axis_text_y=element_text(size=6))\n",
    "    + theme_figsize('square')  # Faster\n",
    "    # + theme_figsize('half')\n",
    "    # + theme_figsize('full')\n",
    "    # + theme_figsize('full_dense')\n",
    "    + ggtitle(rf'Coverage error over fold$\\times$instance, by params')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model diagnostics: one model, all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_i = -1\n",
    "params = param_list[params_i]\n",
    "print(\"param_list:\\n%s\" % '\\n'.join('  %s: %r' % (i, x) for i, x in enumerate(param_list)))\n",
    "print()\n",
    "print('models:')\n",
    "print('  params[%s/%s]: %r' % (params_i, len(param_list), params))\n",
    "print('  fold[*/%s]' % cv.cv.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.759s"
   },
   "outputs": [],
   "source": [
    "# Coverage errors: one model, all folds\n",
    "#   - Subset: models.params == params\n",
    "#   - Grain: sum(recs[model.test_i] for model)\n",
    "#   - Dims: model_id, params, fold, y_true, rec_id\n",
    "#   - Metrics: coverage_error\n",
    "coverage_errors_one_all = (coverage_errors_all_all\n",
    "    [lambda df: df.params == params]  # One model, all folds\n",
    ")\n",
    "display(\n",
    "    df_summary(coverage_errors_one_all).T,\n",
    "    coverage_errors_one_all[:10],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "24.91s"
   },
   "outputs": [],
   "source": [
    "#   - Subset: models.params == params\n",
    "#   - Grain: sum(recs[model.test_i].groupby(y_true) for model)\n",
    "#       - Over: fold, rec_id\n",
    "#   - Dims: y_true\n",
    "#   - Metrics: count, coverage_error.percentiles\n",
    "# in: coverage_errors_one_all, recs\n",
    "(coverage_errors_one_all\n",
    "    .astype({'y_true': metadata.species.df.shorthand.dtype})\n",
    "    .pipe(df_reverse_cat, 'y_true')\n",
    "    .pipe(ggplot, aes(x='y_true', y='coverage_error'))\n",
    "    + geom_hline(yintercept=1, color='grey')\n",
    "    + geom_hline(yintercept=recs.species.nunique(), color='grey')\n",
    "    + geom_count(aes(size='..n..'), alpha=1)  # n (count) instead of prop (proportion)\n",
    "    + scale_size_area()  # Instead of default scale_size(), because it's a count [I don't grok this but it looks good]\n",
    "    + geom_point(stat='summary', fun_y=np.median, alpha=1, color='red', shape='|', size=6, stroke=2)\n",
    "    + coord_flip()\n",
    "    # + theme_figsize('inline')\n",
    "    # + theme_figsize('square')\n",
    "    # + theme_figsize('half')\n",
    "    + theme_figsize('half_dense')\n",
    "    # + theme_figsize('full')\n",
    "    # + theme_figsize('full_dense')\n",
    "    + ggtitle(rf'Coverage error over fold$\\times$instance ({params})')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.286s"
   },
   "outputs": [],
   "source": [
    "# One-model/all-folds confusion matrix\n",
    "with figsize(\n",
    "    # 'square',\n",
    "    # 'full',\n",
    "    'full_dense',\n",
    "):\n",
    "    (cv_models\n",
    "        [lambda df: df.params == params]\n",
    "        .pipe(lambda df: plot_confusion_matrix(\n",
    "            classes=df.iloc[0].test_evals.classes,\n",
    "            M=np.array([\n",
    "                row.test_evals.confusion_matrix_prob()\n",
    "                for row in df_rows(df)\n",
    "            ]).sum(axis=0),\n",
    "            # normalize=False,  # For counts\n",
    "            raw=True, scale=10,  # Faster dev\n",
    "            format=None,  # Omit numbers, too dense\n",
    "        ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model diagnostics: one model, one fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "# params_i = ...  # Comment out to reuse from above\n",
    "params = param_list[params_i]\n",
    "[(_, model)] = list(cv_models[lambda df: (df.params == params) & (df.fold == fold)].iterrows())\n",
    "print(\"param_list:\\n%s\" % '\\n'.join('  %s: %r' % (i, x) for i, x in enumerate(param_list)))\n",
    "print()\n",
    "print('model:')\n",
    "print('  params[%s/%s]: %r' % (params_i, len(param_list), model.params))\n",
    "print('  fold[%s/%s]' % (model.fold, cv.cv.n_splits))\n",
    "print()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# in: model\n",
    "model_id = model.model_id\n",
    "params = model.params\n",
    "fold = model.fold\n",
    "# train_evals = model.train_evals\n",
    "test_evals = model.test_evals\n",
    "\n",
    "# in: model, recs\n",
    "# train_recs = recs.iloc[train_evals.i]\n",
    "# train_X = Search.X(recs)[train_evals.i]\n",
    "# train_y = Search.y(recs)[train_evals.i]\n",
    "test_recs = recs.iloc[test_evals.i]\n",
    "test_X = Search.X(recs)[test_evals.i]\n",
    "test_y = Search.y(recs)[test_evals.i]  # (Don't need to store cv_models.test_evals.y if we have recs -- which sometimes we don't?)\n",
    "\n",
    "display(\n",
    "    # len(train_recs),\n",
    "    len(test_recs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# TODO TODO Restore this plot like 'Coverage error over ...' above, so we can see _one_ model instead of aggregating over n_splits models\n",
    "# # TODO Update [kill the .merge, then species -> y_true]\n",
    "# (search.coverage_error_by(test_recs, 'id')\n",
    "#     [:5]\n",
    "#     # .merge(test_recs[['id', 'species']], on='id', how='left')\n",
    "#     # .pipe(ggplot, aes(x='species', y='coverage_error'))\n",
    "#     # + geom_count(aes(size='..n..'))\n",
    "#     # + stat_summary(fun_y=np.median, geom='point', color='red', alpha=.5, shape='|', size=6, stroke=1)\n",
    "#     # + stat_summary(\n",
    "#     #     fun_ymin=partial(np.percentile, q=25), fun_ymax=partial(np.percentile, q=75),\n",
    "#     #     geom='linerange', color='red', alpha=.5, size=1,\n",
    "#     # )\n",
    "#     # + coord_flip()\n",
    "#     # + geom_hline(yintercept=len(search.classes_), color='grey')\n",
    "#     # + scale_x_discrete(limits=list(reversed(test_recs.species.cat.categories)))\n",
    "#     # + theme_figsize('square')\n",
    "#     # + ggtitle(rf'Coverage error over instance ({model_id})')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# One-model/one-fold confusion matrix\n",
    "with figsize(\n",
    "    'square',\n",
    "    # 'full',\n",
    "    # 'full_dense',\n",
    "):\n",
    "    plot_confusion_matrix_df(\n",
    "        confusion_matrix_prob_df(model.test_evals.y, model.test_evals.y_scores, model.test_evals.classes),\n",
    "        title=model.model_id,\n",
    "        # normalize=False,  # For counts\n",
    "        raw=True, scale=10,  # Faster dev\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubo-features (PYTHONSTARTUP)",
   "language": "python",
   "name": "bubo-features (PYTHONSTARTUP)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
