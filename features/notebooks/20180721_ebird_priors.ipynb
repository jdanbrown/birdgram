{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "- [x] Get ebird data (US only) from Alex\n",
    "- [ ] ebird.predict_proba(loc, date) -> probs\n",
    "    - [ ] Figure out how to aggregate to make data fit comfortably in mem\n",
    "    - [ ] Make an out-of-core ddf pass over the cols-for-time-space-priors.tsv file to produce the aggregates\n",
    "    - [ ] Use the aggregates in memory to power the priors model\n",
    "- [ ] Ensemble audio probs + ebird probs (expose tuning param for weighted combination)\n",
    "- [ ] See how it improves model validation (e.g. many species should be way closer to few species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.57s"
   },
   "outputs": [],
   "source": [
    "import geohash\n",
    "from notebooks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a global dask progress bar\n",
    "dask_progress().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -> potoo.dask\n",
    "def dd_read_parquet_sample(\n",
    "    path,\n",
    "    sample: Union[float, List[float]] = None,\n",
    "    sample_divisions=None,\n",
    "    sample_npartitions=None,\n",
    "    sample_repartition_force=False,\n",
    "    sample_to_parquet=dict(compression='gzip'),\n",
    "    sample_get=None,  # Default: dask_get_for_scheduler('threads')\n",
    "    random_state=0,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    dd.read_parquet with sampling, to make it easy to downsample large files for faster dev iteration\n",
    "    - Same as dd.read_parquet(path, **kwargs) if sample isn't given\n",
    "    \"\"\"\n",
    "    if sample:\n",
    "        if isinstance(sample, float):\n",
    "            sample = [sample]\n",
    "        in_path = _sample_path(path, *sample[:-1])\n",
    "        out_path = _sample_path(path, *sample)\n",
    "        if not (Path(out_path) / '_metadata').exists():\n",
    "            log.info('Caching sample: %s <- %s' % (\n",
    "                Path(out_path).relative_to(os.getcwd()),\n",
    "                Path(in_path).relative_to(os.getcwd())),\n",
    "            )\n",
    "            # Read and sample\n",
    "            ddf = (\n",
    "                dd.read_parquet(in_path, **kwargs)\n",
    "                .sample(frac=sample[-1], replace=False, random_state=random_state)\n",
    "            )\n",
    "            # Repartition, if requested\n",
    "            if sample_divisions or sample_npartitions:\n",
    "                ddf = ddf.repartition(\n",
    "                    divisions=sample_divisions,\n",
    "                    npartitions=sample_npartitions,\n",
    "                    force=sample_repartition_force,\n",
    "                )\n",
    "            # Write cached sample\n",
    "            #   - Use 'threads' if repartitioning, else 'processes', to avoid ipc bottlenecks from the shuffle\n",
    "            sample_get = sample_get or dask_get_for_scheduler('threads' if sample_npartitions else 'processes')\n",
    "            (ddf\n",
    "                .to_parquet(out_path, **sample_to_parquet, compute=False)\n",
    "                .compute(get=sample_get)\n",
    "            )\n",
    "        # log.debug('Reading cached sample: %s' % Path(out_path).relative_to(os.getcwd()))\n",
    "        path = out_path\n",
    "    return dd.read_parquet(path, **kwargs)\n",
    "\n",
    "def _sample_path(path: str, *sample: float) -> str:\n",
    "    return '-'.join([path, *map(str, sample)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample raw data (in shell)\n",
    "#   (\n",
    "#     set -eux\n",
    "#     set -o pipefail\n",
    "#     raw_f='ebd_US_relFeb-2017.txt.gz'\n",
    "#     # 147m lines -> 14.7m lines\n",
    "#     in_f=\"$raw_f\"; out_f=\"$raw_f-0.1\"\n",
    "#     cat \"$in_f\" \\\n",
    "#       | pv -terb -s\"`du -hs \"$in_f\" | field 0`\" -cN in \\\n",
    "#       | gunzip \\\n",
    "#       | sample-lines .1 --seed=0 --keep-header \\\n",
    "#       | pigz \\\n",
    "#       | pv -terb -cN out \\\n",
    "#       > \"$out_f\"\n",
    "#     # 14.7m lines -> 1.47m lines\n",
    "#     in_f=\"$raw_f-0.1\"; out_f=\"$raw_f-0.01\"\n",
    "#     cat \"$in_f\" \\\n",
    "#       | pv -terb -s\"`du -hs \"$in_f\" | field 0`\" -cN in \\\n",
    "#       | gunzip \\\n",
    "#       | sample-lines .1 --seed=0 --keep-header \\\n",
    "#       | pigz \\\n",
    "#       | pv -terb -cN out \\\n",
    "#       > \"$out_f\"\n",
    "#     # 1.47m lines -> 147k lines\n",
    "#     in_f=\"$raw_f-0.01\"; out_f=\"$raw_f-0.001\"\n",
    "#     cat \"$in_f\" \\\n",
    "#       | pv -terb -s\"`du -hs \"$in_f\" | field 0`\" -cN in \\\n",
    "#       | gunzip \\\n",
    "#       | sample-lines .1 --seed=0 --keep-header \\\n",
    "#       | pigz \\\n",
    "#       | pv -terb -cN out \\\n",
    "#       > \"$out_f\"\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.181s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'9.9 MB'"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "text/plain": "'1.5 MB'"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>dtype</th>\n      <th>sizeof</th>\n      <th>len</th>\n      <th>count</th>\n      <th>nunique</th>\n      <th>mean</th>\n      <th>std</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>CATEGORY</th>\n      <th>object</th>\n      <th>559319</th>\n      <th>10000</th>\n      <th>10000</th>\n      <th>7</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">domestic</div></td>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">spuh</div></td>\n    </tr>\n    <tr>\n      <th>SCIENTIFIC NAME</th>\n      <th>object</th>\n      <th>666281</th>\n      <th>10000</th>\n      <th>10000</th>\n      <th>608</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">Acanthis flammea</div></td>\n      <td><div class=\"not-number\">Charadrius semipalmatus</div></td>\n      <td><div class=\"not-number\">Lophodytes cucullatus</div></td>\n      <td><div class=\"not-number\">Sayornis phoebe</div></td>\n      <td><div class=\"not-number\">Zosterops japonicus</div></td>\n    </tr>\n    <tr>\n      <th>OBSERVATION COUNT</th>\n      <th>object</th>\n      <th>571101</th>\n      <th>10000</th>\n      <th>10000</th>\n      <th>163</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">1</div></td>\n      <td><div class=\"not-number\">1</div></td>\n      <td><div class=\"not-number\">3</div></td>\n      <td><div class=\"not-number\">X</div></td>\n      <td><div class=\"not-number\">X</div></td>\n    </tr>\n    <tr>\n      <th>COUNTY CODE</th>\n      <th>object</th>\n      <th>579150</th>\n      <th>10000</th>\n      <th>9975</th>\n      <th>1316</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">US-AK-013</div></td>\n      <td><div class=\"not-number\">US-FL-105</div></td>\n      <td><div class=\"not-number\">US-MI-077</div></td>\n      <td><div class=\"not-number\">US-OR-003</div></td>\n      <td><div class=\"not-number\">US-WY-039</div></td>\n    </tr>\n    <tr>\n      <th>LATITUDE</th>\n      <th>float64</th>\n      <th>240000</th>\n      <th>10000</th>\n      <th>10000</th>\n      <th>5304</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td>19.1</td>\n      <td>35.6</td>\n      <td>39.8</td>\n      <td>42.3</td>\n      <td>71.4</td>\n    </tr>\n    <tr>\n      <th>LONGITUDE</th>\n      <th>float64</th>\n      <th>240000</th>\n      <th>10000</th>\n      <th>10000</th>\n      <th>5303</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td>-172</td>\n      <td>-107</td>\n      <td>-88.1</td>\n      <td>-77.7</td>\n      <td>173</td>\n    </tr>\n    <tr>\n      <th>OBSERVATION DATE</th>\n      <th>object</th>\n      <th>590000</th>\n      <th>10000</th>\n      <th>10000</th>\n      <th>5557</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">1800-01-01</div></td>\n      <td><div class=\"not-number\">1989-04-30</div></td>\n      <td><div class=\"not-number\">1998-03-15</div></td>\n      <td><div class=\"not-number\">2002-06-15</div></td>\n      <td><div class=\"not-number\">2004-12-31</div></td>\n    </tr>\n    <tr>\n      <th>SAMPLING EVENT IDENTIFIER</th>\n      <th>object</th>\n      <th>575180</th>\n      <th>10000</th>\n      <th>10000</th>\n      <th>9883</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">S1000381</div></td>\n      <td><div class=\"not-number\">S16318134</div></td>\n      <td><div class=\"not-number\">S25555972</div></td>\n      <td><div class=\"not-number\">S4519111</div></td>\n      <td><div class=\"not-number\">S9998747</div></td>\n    </tr>\n    <tr>\n      <th>ALL SPECIES REPORTED</th>\n      <th>int64</th>\n      <th>270900</th>\n      <th>10000</th>\n      <th>10000</th>\n      <th>2</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "                                                                                    min                      25%                    50%              75%                  max\n                          dtype   sizeof len   count nunique mean std                                                                                                        \nCATEGORY                  object  559319 10000 10000 7       NaN  NaN          domestic                  species                species          species                 spuh\nSCIENTIFIC NAME           object  666281 10000 10000 608     NaN  NaN  Acanthis flammea  Charadrius semipalmatus  Lophodytes cucullatus  Sayornis phoebe  Zosterops japonicus\nOBSERVATION COUNT         object  571101 10000 10000 163     NaN  NaN                 1                        1                      3                X                    X\nCOUNTY CODE               object  579150 10000 9975  1316    NaN  NaN         US-AK-013                US-FL-105              US-MI-077        US-OR-003            US-WY-039\nLATITUDE                  float64 240000 10000 10000 5304    NaN  NaN              19.1                     35.6                   39.8             42.3                 71.4\nLONGITUDE                 float64 240000 10000 10000 5303    NaN  NaN              -172                     -107                  -88.1            -77.7                  173\nOBSERVATION DATE          object  590000 10000 10000 5557    NaN  NaN        1800-01-01               1989-04-30             1998-03-15       2002-06-15           2004-12-31\nSAMPLING EVENT IDENTIFIER object  575180 10000 10000 9883    NaN  NaN          S1000381                S16318134              S25555972         S4519111             S9998747\nALL SPECIES REPORTED      int64   270900 10000 10000 2       NaN  NaN                 0                        1                      1                1                    1"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    }
   ],
   "source": [
    "# Inspect raw data\n",
    "# ebd_tsv_path = f'{data_dir}/ebird/ebd_US_relFeb-2017.txt.gz'        #  18gb,  147m lines\n",
    "# ebd_tsv_path = f'{data_dir}/ebird/ebd_US_relFeb-2017.txt.gz-0.1'    # 778mb, 14.7m lines\n",
    "# ebd_tsv_path = f'{data_dir}/ebird/ebd_US_relFeb-2017.txt.gz-0.01'   # 100mb, 1.47m lines\n",
    "ebd_tsv_path = f'{data_dir}/ebird/ebd_US_relFeb-2017.txt.gz-0.001'  #  12mb,  147k lines\n",
    "proj_cols = [\n",
    "    'CATEGORY', 'SCIENTIFIC NAME', 'OBSERVATION COUNT', 'COUNTY CODE', 'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE',\n",
    "    'SAMPLING EVENT IDENTIFIER', 'ALL SPECIES REPORTED',\n",
    "]\n",
    "ebd_tsv_df = pd.read_csv(ebd_tsv_path, sep='\\t', nrows=10000, compression='gzip')\n",
    "display(\n",
    "    humanize.naturalsize(len(joblib_dumps(ebd_tsv_df))),\n",
    "    humanize.naturalsize(len(joblib_dumps(ebd_tsv_df[proj_cols]))),\n",
    "    # df_summary(ebd_tsv_df).T,\n",
    "    df_summary(ebd_tsv_df[proj_cols]).T,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ebd.txt -> proj.tsv (in shell)\n",
    "#   - ~10x faster to read than raw data: no gunzip + fewer cols\n",
    "#   - TODO Re-run for suf=1 (no downsampling) to populate group_identifier (col 40)\n",
    "#   (\n",
    "#     set -eux\n",
    "#     set -o pipefail\n",
    "#     sufs=(\n",
    "#       0.001\n",
    "#       0.01\n",
    "#       0.1\n",
    "#       # 1\n",
    "#     )\n",
    "#     for suf in \"${sufs[@]}\"; do\n",
    "#       in_f=\"ebd_US_relFeb-2017.txt.gz$suf\"\n",
    "#       out_f=\"derived/priors/ebd_US_relFeb-2017-0-proj.tsv-$suf\"\n",
    "#       cat \"$in_f\" \\\n",
    "#         | pv -terb -cN in -s\"`du -hs \"$in_f\" | field 0`\" \\\n",
    "#         | gunzip \\\n",
    "#         | cut -f4,6,9,17,25,26,27,32,39,40 \\\n",
    "#         | pv -terb -cN out \\\n",
    "#         > \"$out_f\"\n",
    "#     done\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(np.vectorize, otypes=[str])\n",
    "def np_geohash_encode_safe(lat, lon, **kwargs):\n",
    "    if pd.isnull(lat):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return geohash.encode(lat, lon, **kwargs)\n",
    "\n",
    "@partial(np.vectorize, otypes=[np.uint64])\n",
    "def np_geohash_encode_uint64_safe(lat, lon, **kwargs):\n",
    "    if pd.isnull(lat):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return geohash.encode_uint64(lat, lon, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TODO Prototype e2e pipeline in one go, using downsampled data\n",
    "#   - Output: priors\n",
    "# sample = 1      #  147m lines\n",
    "# sample = 0.1    # 14.7m lines\n",
    "# sample = 0.01   # 1.47m lines\n",
    "sample = 0.001  #  147k lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "4.82s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[                                                                                          ] | 0% Completed |  0.0s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#                                                                                         ] | 1% Completed |  0.1s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[####                                                                                      ] | 5% Completed |  0.2s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[######                                                                                    ] | 7% Completed |  0.3s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#########                                                                                 ] | 10% Completed |  0.4s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[###########                                                                               ] | 12% Completed |  0.5s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[##############                                                                            ] | 15% Completed |  0.7s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[################                                                                          ] | 18% Completed |  0.8s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[##################                                                                        ] | 21% Completed |  0.9s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#####################                                                                     ] | 24% Completed |  1.0s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[########################                                                                  ] | 27% Completed |  1.1s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[###########################                                                               ] | 30% Completed |  1.2s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#############################                                                             ] | 32% Completed |  1.3s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[################################                                                          ] | 36% Completed |  1.4s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[###################################                                                       ] | 39% Completed |  1.5s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[######################################                                                    ] | 42% Completed |  1.6s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#########################################                                                 ] | 45% Completed |  1.8s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[############################################                                              ] | 49% Completed |  1.9s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[##############################################                                            ] | 51% Completed |  2.0s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#################################################                                         ] | 54% Completed |  2.1s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[###################################################                                       ] | 57% Completed |  2.2s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[######################################################                                    ] | 60% Completed |  2.3s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#########################################################                                 ] | 63% Completed |  2.4s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[###########################################################                               ] | 66% Completed |  2.5s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[###############################################################                           ] | 70% Completed |  2.6s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#################################################################                         ] | 73% Completed |  2.7s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#####################################################################                     ] | 76% Completed |  2.8s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#######################################################################                   ] | 79% Completed |  2.9s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[###########################################################################               ] | 83% Completed |  3.0s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[##############################################################################            ] | 87% Completed |  3.2s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#################################################################################         ] | 91% Completed |  3.3s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[####################################################################################      ] | 94% Completed |  3.4s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#######################################################################################   ] | 97% Completed |  3.5s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[##########################################################################################] | 100% Completed |  3.6s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[                                                                                          ] | 0% Completed |  0.0s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[##########################################################################################] | 100% Completed |  0.1s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[                                                                                          ] | 0% Completed |  0.0s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[##########################################################################################] | 100% Completed |  0.1s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "text/plain": "124"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "text/plain": "142242"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>scientific_name</th>\n      <th>observation_count</th>\n      <th>county_code</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>observation_date</th>\n      <th>all_species_reported</th>\n      <th>checklist_id</th>\n      <th>geohash4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">Sterna forsteri</div></td>\n      <td><div class=\"not-number\">12</div></td>\n      <td><div class=\"not-number\">US-CA-087</div></td>\n      <td>36.9</td>\n      <td>-122.0</td>\n      <td><div class=\"not-number\">1996-07-03</div></td>\n      <td>0.0</td>\n      <td>14820275</td>\n      <td><div class=\"not-number\">9q93</div></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">Larus delawarensis</div></td>\n      <td><div class=\"not-number\">1</div></td>\n      <td><div class=\"not-number\">US-OH-171</div></td>\n      <td>41.5</td>\n      <td>-84.5</td>\n      <td><div class=\"not-number\">2004-08-13</div></td>\n      <td>0.0</td>\n      <td>4066497</td>\n      <td><div class=\"not-number\">dp7g</div></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">Sayornis saya</div></td>\n      <td><div class=\"not-number\">1</div></td>\n      <td><div class=\"not-number\">US-CA-087</div></td>\n      <td>36.9</td>\n      <td>-122.0</td>\n      <td><div class=\"not-number\">1988-11-02</div></td>\n      <td>0.0</td>\n      <td>12524570</td>\n      <td><div class=\"not-number\">9q93</div></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">Cardinalis cardinalis</div></td>\n      <td><div class=\"not-number\">X</div></td>\n      <td><div class=\"not-number\">US-NE-067</div></td>\n      <td>40.3</td>\n      <td>-96.8</td>\n      <td><div class=\"not-number\">1968-03-30</div></td>\n      <td>1.0</td>\n      <td>10521006</td>\n      <td><div class=\"not-number\">9z5j</div></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">Anas discors</div></td>\n      <td><div class=\"not-number\">3</div></td>\n      <td><div class=\"not-number\">US-IL-097</div></td>\n      <td>42.4</td>\n      <td>-88.1</td>\n      <td><div class=\"not-number\">1997-08-13</div></td>\n      <td>1.0</td>\n      <td>11895058</td>\n      <td><div class=\"not-number\">dp93</div></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">Aythya marila</div></td>\n      <td><div class=\"not-number\">18</div></td>\n      <td><div class=\"not-number\">US-CA-087</div></td>\n      <td>36.9</td>\n      <td>-122.0</td>\n      <td><div class=\"not-number\">1995-01-24</div></td>\n      <td>0.0</td>\n      <td>11638008</td>\n      <td><div class=\"not-number\">9q93</div></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">Setophaga citrina</div></td>\n      <td><div class=\"not-number\">1</div></td>\n      <td><div class=\"not-number\">US-AR-099</div></td>\n      <td>33.7</td>\n      <td>-93.1</td>\n      <td><div class=\"not-number\">2004-07-13</div></td>\n      <td>0.0</td>\n      <td>12124092</td>\n      <td><div class=\"not-number\">9vvz</div></td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">Leucophaeus atricilla</div></td>\n      <td><div class=\"not-number\">2</div></td>\n      <td><div class=\"not-number\">US-MD-019</div></td>\n      <td>38.6</td>\n      <td>-75.7</td>\n      <td><div class=\"not-number\">2000-04-29</div></td>\n      <td>0.0</td>\n      <td>-554362</td>\n      <td><div class=\"not-number\">dqf5</div></td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">Larus delawarensis</div></td>\n      <td><div class=\"not-number\">1</div></td>\n      <td><div class=\"not-number\">US-VA-810</div></td>\n      <td>37.0</td>\n      <td>-76.1</td>\n      <td><div class=\"not-number\">2001-02-11</div></td>\n      <td>0.0</td>\n      <td>5669109</td>\n      <td><div class=\"not-number\">dq9f</div></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">Icterus spurius</div></td>\n      <td><div class=\"not-number\">X</div></td>\n      <td><div class=\"not-number\">US-IL-031</div></td>\n      <td>42.1</td>\n      <td>-87.8</td>\n      <td><div class=\"not-number\">1966-05-22</div></td>\n      <td>1.0</td>\n      <td>12340445</td>\n      <td><div class=\"not-number\">dp3x</div></td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "  category        scientific_name observation_count county_code  latitude  longitude observation_date  all_species_reported  checklist_id geohash4\n0  species        Sterna forsteri                12   US-CA-087    36.853   -121.809       1996-07-03                   0.0      14820275     9q93\n1  species     Larus delawarensis                 1   US-OH-171    41.457    -84.524       2004-08-13                   0.0       4066497     dp7g\n2  species          Sayornis saya                 1   US-CA-087    36.853   -121.809       1988-11-02                   0.0      12524570     9q93\n3  species  Cardinalis cardinalis                 X   US-NE-067    40.287    -96.834       1968-03-30                   1.0      10521006     9z5j\n4  species           Anas discors                 3   US-IL-097    42.402    -88.083       1997-08-13                   1.0      11895058     dp93\n5  species          Aythya marila                18   US-CA-087    36.853   -121.809       1995-01-24                   0.0      11638008     9q93\n6  species      Setophaga citrina                 1   US-AR-099    33.675    -93.123       2004-07-13                   0.0      12124092     9vvz\n7  species  Leucophaeus atricilla                 2   US-MD-019    38.644    -75.717       2000-04-29                   0.0       -554362     dqf5\n8  species     Larus delawarensis                 1   US-VA-810    37.025    -76.089       2001-02-11                   0.0       5669109     dq9f\n9  species        Icterus spurius                 X   US-IL-031    42.116    -87.771       1966-05-22                   1.0      12340445     dp3x"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>dtype</th>\n      <th>sizeof</th>\n      <th>len</th>\n      <th>count</th>\n      <th>nunique</th>\n      <th>mean</th>\n      <th>std</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>category</th>\n      <th>category</th>\n      <th>55899</th>\n      <th>1000</th>\n      <th>1000</th>\n      <th>6</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">domestic</div></td>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">species</div></td>\n      <td><div class=\"not-number\">spuh</div></td>\n    </tr>\n    <tr>\n      <th>scientific_name</th>\n      <th>category</th>\n      <th>66477</th>\n      <th>1000</th>\n      <th>1000</th>\n      <th>323</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">Accipiter cooperii</div></td>\n      <td><div class=\"not-number\">Circus cyaneus</div></td>\n      <td><div class=\"not-number\">Megaceryle alcyon</div></td>\n      <td><div class=\"not-number\">Riparia riparia</div></td>\n      <td><div class=\"not-number\">Zosterops japonicus</div></td>\n    </tr>\n    <tr>\n      <th>observation_count</th>\n      <th>category</th>\n      <th>57139</th>\n      <th>1000</th>\n      <th>1000</th>\n      <th>61</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">1</div></td>\n      <td><div class=\"not-number\">1</div></td>\n      <td><div class=\"not-number\">3</div></td>\n      <td><div class=\"not-number\">X</div></td>\n      <td><div class=\"not-number\">X</div></td>\n    </tr>\n    <tr>\n      <th>county_code</th>\n      <th>category</th>\n      <th>58000</th>\n      <th>1000</th>\n      <th>1000</th>\n      <th>342</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">US-AK-261</div></td>\n      <td><div class=\"not-number\">US-DE-001</div></td>\n      <td><div class=\"not-number\">US-MA-021</div></td>\n      <td><div class=\"not-number\">US-OH-035</div></td>\n      <td><div class=\"not-number\">US-WY-013</div></td>\n    </tr>\n    <tr>\n      <th>latitude</th>\n      <th>float32</th>\n      <th>24000</th>\n      <th>1000</th>\n      <th>1000</th>\n      <th>581</th>\n      <th>3.845e+01</th>\n      <th>5.196e+00</th>\n      <td>19.6</td>\n      <td>35.5</td>\n      <td>39.4</td>\n      <td>42.3</td>\n      <td>62.9</td>\n    </tr>\n    <tr>\n      <th>longitude</th>\n      <th>float32</th>\n      <th>24000</th>\n      <th>1000</th>\n      <th>1000</th>\n      <th>582</th>\n      <th>-9.569e+01</th>\n      <th>1.768e+01</th>\n      <td>-156</td>\n      <td>-114</td>\n      <td>-88.1</td>\n      <td>-82.3</td>\n      <td>-67.1</td>\n    </tr>\n    <tr>\n      <th>observation_date</th>\n      <th>category</th>\n      <th>59000</th>\n      <th>1000</th>\n      <th>1000</th>\n      <th>897</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">1891-04-23</div></td>\n      <td><div class=\"not-number\">1990-04-19</div></td>\n      <td><div class=\"not-number\">1999-02-06</div></td>\n      <td><div class=\"not-number\">2002-09-28</div></td>\n      <td><div class=\"not-number\">2004-12-31</div></td>\n    </tr>\n    <tr>\n      <th>all_species_reported</th>\n      <th>float16</th>\n      <th>24000</th>\n      <th>1000</th>\n      <th>1000</th>\n      <th>2</th>\n      <th>7.842e-01</th>\n      <th>4.116e-01</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>checklist_id</th>\n      <th>int32</th>\n      <th>28000</th>\n      <th>1000</th>\n      <th>1000</th>\n      <th>983</th>\n      <th>1.016e+07</th>\n      <th>8.642e+06</th>\n      <td>-2264427</td>\n      <td>2527863</td>\n      <td>8740569</td>\n      <td>14560999</td>\n      <td>35084168</td>\n    </tr>\n    <tr>\n      <th>geohash4</th>\n      <th>category</th>\n      <th>53000</th>\n      <th>1000</th>\n      <th>1000</th>\n      <th>446</th>\n      <th>NaN</th>\n      <th>NaN</th>\n      <td><div class=\"not-number\">8e3p</div></td>\n      <td><div class=\"not-number\">9tnr</div></td>\n      <td><div class=\"not-number\">djf8</div></td>\n      <td><div class=\"not-number\">dp9s</div></td>\n      <td><div class=\"not-number\">f800</div></td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "                                                                                             min             25%                50%              75%                  max\n                     dtype    sizeof len  count nunique mean       std                                                                                                   \ncategory             category 55899  1000 1000  6       NaN        NaN                  domestic         species            species          species                 spuh\nscientific_name      category 66477  1000 1000  323     NaN        NaN        Accipiter cooperii  Circus cyaneus  Megaceryle alcyon  Riparia riparia  Zosterops japonicus\nobservation_count    category 57139  1000 1000  61      NaN        NaN                         1               1                  3                X                    X\ncounty_code          category 58000  1000 1000  342     NaN        NaN                 US-AK-261       US-DE-001          US-MA-021        US-OH-035            US-WY-013\nlatitude             float32  24000  1000 1000  581      3.845e+01 5.196e+00                19.6            35.5               39.4             42.3                 62.9\nlongitude            float32  24000  1000 1000  582     -9.569e+01 1.768e+01                -156            -114              -88.1            -82.3                -67.1\nobservation_date     category 59000  1000 1000  897     NaN        NaN                1891-04-23      1990-04-19         1999-02-06       2002-09-28           2004-12-31\nall_species_reported float16  24000  1000 1000  2        7.842e-01 4.116e-01                   0               1                  1                1                    1\nchecklist_id         int32    28000  1000 1000  983      1.016e+07 8.642e+06            -2264427         2527863            8740569         14560999             35084168\ngeohash4             category 53000  1000 1000  446     NaN        NaN                      8e3p            9tnr               djf8             dp9s                 f800"
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    }
   ],
   "source": [
    "ebd_proj_size = 12 * 1024**3\n",
    "ebd_proj_npartitions = 128\n",
    "ebd_proj = (\n",
    "    # Read proj.tsv (raw lines with subset of cols)\n",
    "    #   - sample=0.001: 147k rows, 12mb\n",
    "    dd.read_csv(\n",
    "        f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-0-proj.tsv-{sample}',  # 12gb / 1.2gb / 120mb / 12mb\n",
    "        blocksize=int(ebd_proj_size / ebd_proj_npartitions * sample),\n",
    "        sep='\\t',\n",
    "        dtype={\n",
    "            # lonlat decimal precision [https://en.wikipedia.org/wiki/Decimal_degrees]\n",
    "            #   - float32: ~5-6 decimals, ~.1-1m\n",
    "            #   - float16: ~1-2 decimals, ~1-10km\n",
    "            'LATITUDE': 'float32',\n",
    "            'LONGITUDE': 'float32',\n",
    "            'OBSERVATION COUNT': 'str',  # int | 'X'\n",
    "            'ALL SPECIES REPORTED': 'float16',  # Smallest dtype for {1, 0, None}\n",
    "        },\n",
    "    )\n",
    "    # Simplify col names\n",
    "    .rename(columns=lambda c: c.lower().replace(' ', '_'))\n",
    "    # Drop obs with no checklist id\n",
    "    #   - sample=0.001: 147k -> 142k rows\n",
    "    #   - FIXME Some of these nulls are due to misaligned fields (maybe from the tsv split above?)\n",
    "    #   - Not clear what the other nulls are from; try backing them out to the full field set (including group_identifier)\n",
    "    .pipe(lambda ddf: ddf[ddf.sampling_event_identifier.notnull()])\n",
    "    # Replace sampling_event_identifier and group_identifier with checklist_id\n",
    "    #   - Compress from str to int32 ('S10150441' -> 10150441) for ~46% compression (e.g. 24mb -> 13mb)\n",
    "    #   - Map sampling_event_identifier to pos ('S10150441' -> 10150441) and group_identifier to net ('G554362' -> -554362)\n",
    "    #   - Assume no nulls (filtered out above)\n",
    "    .assign(checklist_id=lambda ddf: (ddf\n",
    "        .group_identifier.combine_first(ddf.sampling_event_identifier)\n",
    "        .str.replace('S', '').str.replace('G', '-').astype(np.int32)\n",
    "    ))\n",
    "    .drop(axis=1, labels=['sampling_event_identifier', 'group_identifier'])\n",
    "    # Create geohash4 from (lat, lon)\n",
    "    .pipe(lambda ddf: ddf.map_partitions(\n",
    "        meta=ddf._meta.assign(\n",
    "            geohash4='',\n",
    "        ),\n",
    "        func=lambda df: df.assign(\n",
    "            geohash4=lambda df: np_geohash_encode_safe(df.latitude, df.longitude, precision=4),\n",
    "        ),\n",
    "    ))\n",
    "    # Create categories to compact space usage via dictionary encoding\n",
    "    #   - NOTE Create category dtypes after munging meta, else weird `pd.Index(None)` errors on downstream reads\n",
    "    .astype({\n",
    "        'category': 'category',\n",
    "        'scientific_name': 'category',\n",
    "        'observation_count': 'category',\n",
    "        'county_code': 'category',\n",
    "        'observation_date': 'category',\n",
    "        'geohash4': 'category',  # Max 32**4 = ~1m values -> ~4mb dictionary (separate dictionary per partition df)\n",
    "    })\n",
    "    # Drop duplicate rows\n",
    "    #   - sample=0.001: 142k -> 98.7k rows\n",
    "    #   - The sample=0.001 raw file shows most lines occur once but ~18 lines are repeated ~2500-2800 times; drop these\n",
    "    #   - The raw->proj step threw out cols, which could create false dupes, but (checklist, species) should be unique enough\n",
    "    #   - TODO Bottleneck; maybe merge into the groupby we'll need at the end?\n",
    "    # .pipe(lambda ddf: (ddf\n",
    "    #     .drop_duplicates(\n",
    "    #         subset=['sampling_event_identifier', 'scientific_name'],\n",
    "    #         # split_out=1,                # sample=0.001: 2.1s (default)\n",
    "    #         # split_out=ddf.npartitions,  # sample=0.001: 25.9s (hmm...)\n",
    "    #     )\n",
    "    # ))\n",
    ")\n",
    "display(\n",
    "    ebd_proj.npartitions,\n",
    "    len(ebd_proj),\n",
    "    ebd_proj.head(10),\n",
    "    df_summary(ebd_proj.head(1000)).T,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TODO Old stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.379s"
   },
   "outputs": [],
   "source": [
    "# Inspect proj.tsv\n",
    "# ebd_proj_tsv_path = f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-0-proj.tsv-1'      #  12gb   147m lines\n",
    "# ebd_proj_tsv_path = f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-0-proj.tsv-0.1'    # 1.2gb  14.7m lines\n",
    "# ebd_proj_tsv_path = f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-0-proj.tsv-0.01'   # 120mb, 1.47m lines\n",
    "ebd_proj_tsv_path = f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-0-proj.tsv-0.001'  #  12mb,  147k lines\n",
    "_ebd_proj_tsv_ddf = lambda **kwargs: (\n",
    "    dd.read_csv(ebd_proj_tsv_path, **kwargs, sep='\\t', dtype={\n",
    "        # lonlat decimal precision [https://en.wikipedia.org/wiki/Decimal_degrees]\n",
    "        #   - float32: ~5-6 decimals, ~.1-1m\n",
    "        #   - float16: ~1-2 decimals, ~1-10km\n",
    "        'LATITUDE': 'float32',\n",
    "        'LONGITUDE': 'float32',\n",
    "        'OBSERVATION COUNT': 'str',  # int | 'X'\n",
    "        'ALL SPECIES REPORTED': 'float16',  # Smallest dtype for {1, 0, None}\n",
    "    })\n",
    "    .rename(columns=lambda c: c.lower().replace(' ', '_'))\n",
    ")\n",
    "ebd_proj_tsv_ddf = _ebd_proj_tsv_ddf()\n",
    "display(\n",
    "    ebd_proj_tsv_ddf.npartitions,\n",
    "    ebd_proj_tsv_ddf.head(10),\n",
    "    df_summary(ebd_proj_tsv_ddf.head(1000)).T,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Dedupe sightings by group_identifier (obs from group checklists get repeated |observers| many times):\n",
    "#   .assign(checklist_id=group_identifier or sampling_event_identifier)\n",
    "#   .drop(axis=1, labels=['sampling_event_identifier', 'group_identifier'])\n",
    "#   .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ebd_proj_path = f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-1-proj.parquet'  # 190 parts, gz [~19s read .category]\n",
    "# ebd_proj_path = f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-2-proj.parquet'  # 16 parts [~30s read .category]\n",
    "# ebd_proj_path = f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-3-proj.parquet'  # 190 parts, gz, cats\n",
    "ebd_proj_path = f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-4-proj.parquet'  # 190 parts, gz, cats, geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# Convert proj.tsv -> proj.parquet\n",
    "#   - [5m41s] 1-proj\n",
    "#   - [6m14s] 3-proj\n",
    "#   - [7m15s] 4-proj\n",
    "(\n",
    "    _ebd_proj_tsv_ddf()\n",
    "    # .head(1000, compute=False)  # Faster dev\n",
    "    # Compress checklist id from str to uint32 (e.g. 'S10150441' -> 10150441)\n",
    "    #   - FIXME Barfs on Nones\n",
    "    # .assign(\n",
    "    #     sampling_event_identifier=lambda ddf: ddf.sampling_event_identifier.str[1:].astype(np.uint32),\n",
    "    # )\n",
    "    # Create geohash from (lat, lon)\n",
    "    .pipe(lambda ddf: ddf.map_partitions(\n",
    "        meta=ddf._meta.assign(\n",
    "            geohash='',\n",
    "        ),\n",
    "        func=lambda df: df.assign(\n",
    "            geohash=lambda df: np_geohash_encode_safe(df.latitude, df.longitude),\n",
    "        ),\n",
    "    ))\n",
    "    # Create categories to tell fastparquet to write these columns using dictionary encoding (more compact)\n",
    "    #   - NOTE Create category dtypes after munging meta, else weird `pd.Index(None)` errors on downstream reads\n",
    "    .astype({\n",
    "        'category': 'category',\n",
    "        'scientific_name': 'category',\n",
    "        'observation_count': 'category',\n",
    "        'county_code': 'category',\n",
    "        'observation_date': 'category',\n",
    "        'geohash': 'category',\n",
    "    })\n",
    "    .pipe(puts, lambda ddf: ddf.npartitions)\n",
    "    .pipe(tap, lambda ddf: pp(dict(**ddf.dtypes)))\n",
    "    .to_parquet(ebd_proj_path,\n",
    "        compression='gzip',\n",
    "        # compression=None,\n",
    "        compute=False,\n",
    "    )\n",
    "    # .compute(get=dask_get_for_scheduler('synchronous'))\n",
    "    # .compute(get=dask_get_for_scheduler('threads'))\n",
    "    .compute(get=dask_get_for_scheduler('processes'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "2.257s"
   },
   "outputs": [],
   "source": [
    "# Test reading the file we just wrote\n",
    "(dd.read_parquet(ebd_proj_path)\n",
    "    # .head(1000, compute=False)\n",
    "    # .pipe(tap, lambda ddf: pp(list(ddf.dtypes)))\n",
    "    # .categorize() to upgrade unknown->known categoricals (requires an extra pass over the input)\n",
    "    # .pipe(lambda ddf: ddf.categorize(columns=list(\n",
    "    #     ddf.dtypes[lambda s: s == 'category'].index\n",
    "    #     # ['category', 'scientific_name']\n",
    "    # )))\n",
    "    # .pipe(tap, lambda ddf: pp(list(ddf.dtypes)))\n",
    "    .head(1000, compute=False)\n",
    "    .compute()\n",
    "    .pipe(df_summary).T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TODO Did the above .assign(geohash=...) work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "2.867s"
   },
   "outputs": [],
   "source": [
    "# TODO TODO How big is each col, really? (cf. df_summary above)\n",
    "# Test reading the file we just wrote\n",
    "# cols = dd.read_parquet(ebd_proj_path).columns\n",
    "cols = [\n",
    "    # 'category', 'latitude', 'sampling_event_identifier',\n",
    "    *dd.read_parquet(ebd_proj_path).columns,\n",
    "]\n",
    "for i, col in enumerate(cols):\n",
    "    log.info('col %s/%s: %s' % (i+1, len(cols), col))\n",
    "    (\n",
    "        dd_read_parquet_sample(\n",
    "            ebd_proj_path,\n",
    "            # sample=None,  # 1.47b\n",
    "            # sample=.1, sample_npartitions=8,  # 147m\n",
    "            sample=[.1, .1],  # 14.7m\n",
    "            # sample=[.1, .1, .1],  # 1.47m\n",
    "            # columns=[col],  # XXX\n",
    "        )\n",
    "        # Drop obs with no checklist id\n",
    "        #   - FIXME Some of these nulls are due to misaligned fields (maybe from the tsv split above?)\n",
    "        #   - Not clear what the other nulls are from; try backing them out to the full field set (including group_identifier)\n",
    "        # .pipe(lambda ddf: ddf[ddf.sampling_event_identifier.notnull()])  # ~96.3% (147 -> 142)\n",
    "        # TODO De-dupe by group_identifier (requires re-running pipeline above to add group_identifier)\n",
    "        # Compress checklist id from str (e.g. 'S10150441') to uint32 (e.g. 10150441), for ~46% compression (e.g. 24mb -> 13mb)\n",
    "        #   - Assumes no nulls (filtered out above)\n",
    "        # .assign(sampling_event_identifier=lambda ddf: ddf.sampling_event_identifier.str[1:].astype(np.uint32))\n",
    "        [[col]]\n",
    "        .to_parquet(f'/tmp/junk-ebd-col-{col}.parquet', compute=False,\n",
    "            # compression='gzip',\n",
    "            compression=None,\n",
    "        )\n",
    "        # .compute(get=dask_get_for_scheduler('threads'))\n",
    "        .compute(get=dask_get_for_scheduler('processes'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "2.895s"
   },
   "outputs": [],
   "source": [
    "# TODO Prototype (tail of) pipeline\n",
    "(\n",
    "    dd_read_parquet_sample(ebd_proj_path,\n",
    "        # sample=None,      # 1.47b, len[55s], no_checklist[.037]\n",
    "        # sample=.1,        # 147m,  len[7.1s], no_checklist[.037]\n",
    "        sample=[.1, .1],  # 14.7m, len[1.1s], no_checklist[.037]\n",
    "    )\n",
    "    # Drop obs with no checklist id\n",
    "    #   - FIXME Some of these nulls are due to misaligned fields (maybe from the tsv split above?)\n",
    "    #   - Not clear what the other nulls are from; try backing them out to the full field set (including group_identifier)\n",
    "    .pipe(lambda ddf: ddf[ddf.sampling_event_identifier.notnull()])  # ~96.3% (147 -> 142)\n",
    "    # TODO De-dupe by group_identifier (requires re-running pipeline above to add group_identifier)\n",
    "    # Compress checklist id from str (e.g. 'S10150441') to uint32 (e.g. 10150441), for ~46% compression (e.g. 24mb -> 13mb)\n",
    "    #   - Assumes no nulls (filtered out above)\n",
    "    .assign(sampling_event_identifier=lambda ddf: ddf.sampling_event_identifier.str[1:].astype(np.uint32))\n",
    "    # [:200]\n",
    "    .pipe(len)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.976s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[                                                                                          ] | 0% Completed |  0.0s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#                                                                                         ] | 2% Completed |  0.1s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[##################                                                                        ] | 20% Completed |  0.3s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[###############################                                                           ] | 34% Completed |  0.4s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[##########################################                                                ] | 46% Completed |  0.7s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[#######################################################                                   ] | 61% Completed |  0.8s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[################################################################                          ] | 71% Completed |  1.0s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[####################################################################################      ] | 93% Completed |  1.2s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\r[##########################################################################################] | 100% Completed |  1.3s"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": [
    "# Compress checklist id from str (e.g. 'S10150441') to uint32 (e.g. 10150441), for ~46% compression (e.g. 24mb -> 13mb)\n",
    "#   - Assumes no nulls (filtered out above)\n",
    "col = 'sampling_event_identifier'\n",
    "(dd.read_parquet(f'/tmp/junk-ebd-col-{col}.parquet')\n",
    "    .pipe(lambda ddf: ddf[ddf.sampling_event_identifier.notnull()])\n",
    "    .assign(\n",
    "        sampling_event_identifier=lambda ddf: ddf.sampling_event_identifier.str[1:].astype(np.uint32),\n",
    "    )\n",
    "    .to_parquet(f'/tmp/junk-ebd-col-{col}-uint32.parquet', compute=False, compression=None)\n",
    "    # .to_parquet(f'/tmp/junk-ebd-col-{col}-str.parquet', compute=False, compression=None)\n",
    "    # .query('sampling_event_identifier != sampling_event_identifier')\n",
    "    # [:20]\n",
    "    .compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO TODO Add a .repartition somewhere above so that part files are uniformly sized (currently weirdly bimodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ebd_geo_path = f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-4-geo.parquet'\n",
    "ebd_geo_path = f'{data_dir}/ebird/derived/priors/ebd_US_relFeb-2017-5-geo.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "119.102s"
   },
   "outputs": [],
   "source": [
    "# %%prun -l30\n",
    "# Convert proj.parquet -> geo.parquet\n",
    "#   - 4-geo: geohash:str - 2m29s, 264mb\n",
    "#   - 5-geo: geohash:cat - 1m58s, 219mb\n",
    "(\n",
    "    dd.read_parquet(ebd_proj_path, columns=['latitude', 'longitude'])\n",
    "    # .sample(frac=.1, random_state=0)\n",
    "    # .head(1_000_000, compute=False)\n",
    "    .pipe(lambda ddf: ddf.map_partitions(\n",
    "        meta=ddf._meta.assign(\n",
    "            geohash='',\n",
    "            # geohash=np.uint64(),\n",
    "            # geohash4='',\n",
    "        ),\n",
    "        func=lambda df: df.assign(\n",
    "            # geohash4=None,\n",
    "            # geohash=lambda df: np.vectorize(geohash.encode, [str])(df.latitude, df.longitude),\n",
    "            geohash=lambda df: np_geohash_encode_safe(df.latitude, df.longitude),\n",
    "        ),\n",
    "    ))\n",
    "    # Create categories to tell fastparquet to write these columns using dictionary encoding (more compact)\n",
    "    #   - NOTE Create category dtypes after munging meta, else weird `pd.Index(None)` errors on downstream reads\n",
    "    .astype({\n",
    "        'geohash': 'category',\n",
    "    })\n",
    "    .pipe(puts, lambda ddf: ddf.npartitions)\n",
    "    .pipe(tap, lambda ddf: pp(dict(**ddf.dtypes)))\n",
    "    .to_parquet(ebd_geo_path,\n",
    "        compression='gzip',\n",
    "        # compression=None,\n",
    "        compute=False,\n",
    "    )\n",
    "    # .compute(get=dask_get_for_scheduler('synchronous'))\n",
    "    # .compute(get=dask_get_for_scheduler('threads'))\n",
    "    .compute(get=dask_get_for_scheduler('processes'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.27s"
   },
   "outputs": [],
   "source": [
    "# Test reading the file we just wrote\n",
    "(dd.read_parquet(ebd_proj_path)\n",
    "    # .head(1000, compute=False)\n",
    "    # .pipe(tap, lambda ddf: pp(list(ddf.dtypes)))\n",
    "    # .categorize() to upgrade unknown->known categoricals (requires an extra pass over the input)\n",
    "    # .pipe(lambda ddf: ddf.categorize(columns=list(\n",
    "    #     ddf.dtypes[lambda s: s == 'category'].index\n",
    "    #     # ['category', 'scientific_name']\n",
    "    # )))\n",
    "    # .pipe(tap, lambda ddf: pp(list(ddf.dtypes)))\n",
    "    .head(1000, compute=False)\n",
    "    .compute()\n",
    "    .pipe(df_summary).T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# Count stuff\n",
    "(\n",
    "    dd.read_parquet(ebd_proj_path)\n",
    "    # .head(100_000, compute=False)\n",
    "    # .all_species_reported.size  # 147750723  # FIXME .size (cells) -> len (rows)\n",
    "    # .all_species_reported.count()  # 142317658\n",
    "    # .all_species_reported.isnull().mean()  # .037\n",
    "    # .latitude.isnull().mean()  # .018\n",
    "    .longitude.isnull().mean()  # .018\n",
    "    # .category.nunique()  # 9\n",
    "    # .scientific_name.nunique()  # 1650\n",
    "    # .observation_count.nunique()  # 8813\n",
    "    # .county_code.nunique()  # 3139\n",
    "    # .observation_date.nunique()  # 35722\n",
    "    # .all_species_reported.nunique()  # 2\n",
    "    # .sampling_event_identifier.nunique()  # (Big...)\n",
    "    .compute(get=dask_get_for_scheduler('processes'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "2.946s"
   },
   "outputs": [],
   "source": [
    "# TODO Do the geo agg\n",
    "(\n",
    "    dd.read_parquet(ebd_proj_path, columns=['category', 'geohash', 'scientific_name'])\n",
    "    # .pipe(puts, lambda ddf: ddf.head(10))\n",
    "    # .head(1_000, compute=False).sample(frac=1., random_state=0)  # Faster\n",
    "    # Data volume\n",
    "    #   prec         in     out    time\n",
    "    #      4        100     145    7.0s\n",
    "    #      4      1_000     660    7.0s\n",
    "    #      4     10_000    6404    7.8s\n",
    "    #      4    100_000   50491   18s\n",
    "    #      4  1_000_000  253161  119s\n",
    "    .sample(frac=(\n",
    "        # 100\n",
    "        1_000\n",
    "        # 10_000\n",
    "        # 100_000\n",
    "        # 1_000_000\n",
    "    ) / 147750723, random_state=0)  # Finds more bugs (e.g. nulls)\n",
    "    # Drop obs without location\n",
    "    .dropna(subset=['geohash'])\n",
    "    # Drop obs without a well defined species [https://help.ebird.org/customer/portal/articles/1006825]\n",
    "    .pipe(lambda ddf: ddf[ddf.category.isin(['species', 'domestic', 'issf', 'form'])])\n",
    "    .assign(n=1).groupby(['geohash', 'scientific_name']).n.sum()\n",
    "    # .compute(get=dask_get_for_scheduler('threads'))  # For %debug\n",
    "    .compute(get=dask_get_for_scheduler('processes'))\n",
    "    .reset_index()\n",
    "    .count()\n",
    "    # .pipe(df_summary).T\n",
    "    # [:20]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubo-features (PYTHONSTARTUP)",
   "language": "python",
   "name": "bubo-features (PYTHONSTARTUP)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
