{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.847s"
   },
   "outputs": [],
   "source": [
    "module = 'notebooks._181211_train_cr'\n",
    "exec(f'import {module}; import importlib; importlib.reload({module})')\n",
    "exec(f'from {module} import *')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "4.872s"
   },
   "outputs": [],
   "source": [
    "load_for_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute eval metrics + dims for the plots below\n",
    "#   - Grain: models\n",
    "#   - Metrics: train_score, test_score, ...\n",
    "#   - Dims: model_id, fold, params_data, params_complexity, params_model, ...\n",
    "facet = (\n",
    "    # facet_wrap('params_data_and_model')\n",
    "    facet_grid('params_data ~ params_model')\n",
    "    # facet_grid('params_model ~ params_data')\n",
    "    # facet_grid('n_recs ~ n_species', labeller='label_both')\n",
    ")\n",
    "figsize_width = 12 * 1\n",
    "theme_ = (\n",
    "    geom_blank()\n",
    "    # theme(strip_text_x=element_text(angle=5))\n",
    "    # theme(strip_text_y=element_text(angle=-85))\n",
    ")\n",
    "# FIXME Default theme_gray() plots non-transparent bg, but theme_minimal() reverts back to transparent bg\n",
    "theme_minimal_white = lambda *args, **kwargs: theme_minimal(*args, **kwargs) + theme(plot_background=element_rect('white'))\n",
    "ks_params_data = {\n",
    "    'n_species': 'sp',\n",
    "    'n_recs': 'recs',\n",
    "}\n",
    "ks_params_complexity = [\n",
    "    # 'c_n_estimators',\n",
    "    'c_max_depth',\n",
    "    'c_C',\n",
    "    'c_alpha',\n",
    "    # 'c_class_weight',\n",
    "]\n",
    "abbrev_param_model = lambda k, v: {\n",
    "    'c_cls': '%(v)s',\n",
    "    'c_solver': '%(v)s',\n",
    "    'c_class_weight': '%(v)s',\n",
    "}.get(k, '%(k)s[%(v)s]') % dict(k=k, v=v)\n",
    "# params_model = ...  # Everything else\n",
    "# log.debug('Start')\n",
    "cv_models = (cv_results_splits_df(cv.cv_results_)\n",
    "    # Slow yaml parsing, compute up front\n",
    "    .assign(params_dict=lambda df: df.apply(axis=1, func=lambda row: dict(\n",
    "        **{strip_startswith(k, 'param_'): row[k] for k in df if k.startswith('param_') and k not in ['param_classifier']},\n",
    "        # HACK 'classifier.foo' -> 'c_foo'\n",
    "        #   - TODO Refactor Search.classifier to get rid of the yaml strs ('c_cls', 'c_n_estimators', ...)\n",
    "        **{'c_' + k: v for k, v in yaml.safe_load('{%s}' % row.param_classifier).items()},\n",
    "        # 'classifier': yaml.safe_load('{%s}' % row.param_classifier),\n",
    "    )))\n",
    "    [lambda df: [c for c in df if not c.startswith('param_')]]\n",
    "    # Dims from params (for validation curves, learning curves, etc.)\n",
    "    .assign(\n",
    "        n_species=lambda df: df.params_dict.apply(lambda d: d['n_species']),\n",
    "        n_recs=lambda df: df.params_dict.apply(lambda d: d['n_recs']),\n",
    "        params_data=lambda df: df.params_dict.apply(lambda d: ', '.join(\n",
    "            '%s[%s]' % (k_abbrev, d[k]) for k, k_abbrev in ks_params_data.items() if k in d\n",
    "        )),\n",
    "        params_complexity=lambda df: df.params_dict.apply(lambda d: ', '.join(\n",
    "            '%s[%s]' % (k, d[k]) for k in ks_params_complexity if k in d\n",
    "        )),\n",
    "        params_model=lambda df: df.params_dict.apply(lambda d: ', '.join(\n",
    "            abbrev_param_model(k, d[k]) for k in d if k not in list(ks_params_data) + ks_params_complexity\n",
    "        )),\n",
    "        params_data_and_model=lambda df: df.apply(axis=1, func=lambda row: (\n",
    "            '\\n'.join([row.params_data, row.params_model])\n",
    "        )),\n",
    "        params_model_and_complexity=lambda df: df.apply(axis=1, func=lambda row: (\n",
    "            '\\n'.join([row.params_model, row.params_complexity])\n",
    "        )),\n",
    "    )\n",
    "    # HACK Convert yaml strs ('x: y') to bracket style ('x[y]'), for visual consistency\n",
    "    #   - TODO Refactor Search.classifier to get rid of the yaml strs ('c_cls', 'c_n_estimators', ...)\n",
    "    .applymap(lambda x: x if not isinstance(x, str) else (\n",
    "        re.sub(r'([^][:, ]+):\\s+([^][:,]+)(, )?', r'c_\\1[\\2]\\3',\n",
    "            re.sub(r'classifier\\[([^]]+)\\]', r'\\1',\n",
    "                x,\n",
    "            ),\n",
    "        )\n",
    "    ))\n",
    "    #   - Restore the cats we just destroyed [copied from cv_results_splits_df]\n",
    "    .pipe(df_ordered_cat,\n",
    "        model_id=lambda df: df.model_id.unique(),\n",
    "        params=lambda df: df.params.unique(),\n",
    "        # Order params_data by (sp, recs) descending\n",
    "        params_data=lambda df: sorted(\n",
    "            df.params_data.unique(),\n",
    "            reverse=True,\n",
    "            key=lambda s: [parse.parse('{}[{:g}]', t).fixed for t in s.split(', ')],\n",
    "        ),\n",
    "        # Order params_complexity like c_max_depth\n",
    "        params_complexity=lambda df: sorted(\n",
    "            df.params_complexity.unique(),\n",
    "            reverse=True,  # Match how the normal .unique() would come out (not clear why, and don't care)\n",
    "            key=lambda s: one(\n",
    "                (x['name'], or_else(-np.inf, lambda: float({'None': 'inf'}.get(x['value'], x['value']))))\n",
    "                for x in [\n",
    "                    parse.search('{name}[{value}]', s) or  # Parses first match, ignores rest\n",
    "                    {'name': 'unk', 'value': None}\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        params_model=lambda df: df.params_model.unique(),\n",
    "    )\n",
    "    .pipe(df_ordered_cat,\n",
    "        params_data_and_model=lambda df: (df\n",
    "            .sort_values(['params_data', 'params_model']).params_data_and_model.unique()\n",
    "        ),\n",
    "        params_model_and_complexity=lambda df: (df\n",
    "            .sort_values(['params_model', 'params_complexity']).params_model_and_complexity.unique()\n",
    "        ),\n",
    "    )\n",
    "    # .eval\n",
    "    .assign(\n",
    "        train_evals=lambda df: np.vectorize(SearchEvals)(\n",
    "            i=df.pop('train_i'),\n",
    "            y=df.pop('train_y'),\n",
    "            classes=df['classes'],\n",
    "            y_scores=df.pop('train_predict_proba'),\n",
    "            drop_missing_classes_for_n_species=True,  # HACK Migrate to sk Pipeline to avoid this\n",
    "        ),\n",
    "        test_evals=lambda df: np.vectorize(SearchEvals)(\n",
    "            i=df.pop('test_i'),\n",
    "            y=df.pop('test_y'),\n",
    "            classes=df['classes'],\n",
    "            y_scores=df.pop('test_predict_proba'),\n",
    "            drop_missing_classes_for_n_species=True,  # HACK Migrate to sk Pipeline to avoid this\n",
    "        ),\n",
    "    )\n",
    "    # .pipe(tap, f=lambda df: log.debug('SearchEvals.score'))\n",
    "    .assign(\n",
    "        # HACK Recompute (train_score, test_score) using SearchEvals.score so it can drop_missing_classes_for_n_species\n",
    "        #   - Else you'll get scores that are too low (bad) because they include some -np.inf's in the median\n",
    "        train_score=lambda df: df.train_evals.map(lambda x: x.score()),\n",
    "        train_mean_score=lambda df: df.train_evals.map(lambda x: x.score(agg=np.mean)),\n",
    "        train_50p_score=lambda df: df.train_evals.map(lambda x: x.score(agg=partial(np.percentile, q=50))),\n",
    "        train_75p_score=lambda df: df.train_evals.map(lambda x: x.score(agg=partial(np.percentile, q=75))),\n",
    "        train_95p_score=lambda df: df.train_evals.map(lambda x: x.score(agg=partial(np.percentile, q=95))),\n",
    "        test_score=lambda df: df.test_evals.map(lambda x: x.score()),  # (= 50p)\n",
    "        test_mean_score=lambda df: df.test_evals.map(lambda x: x.score(agg=np.mean)),\n",
    "        test_50p_score=lambda df: df.test_evals.map(lambda x: x.score(agg=partial(np.percentile, q=50))),\n",
    "        test_75p_score=lambda df: df.test_evals.map(lambda x: x.score(agg=partial(np.percentile, q=75))),\n",
    "        test_95p_score=lambda df: df.test_evals.map(lambda x: x.score(agg=partial(np.percentile, q=95))),\n",
    "        # train_score=lambda df: map_progress(lambda x: x.score(), df.train_evals, use='dask', scheduler='threads'),\n",
    "        # test_score=lambda df: map_progress(lambda x: x.score(), df.test_evals, use='dask', scheduler='threads'),\n",
    "    )\n",
    "    .drop(columns=[\n",
    "        'classes',\n",
    "    ])\n",
    "    # proc_stats\n",
    "    # .pipe(tap, f=lambda df: log.debug('proc_stats'))\n",
    "    .pipe(lambda df: df.join(df\n",
    "        .apply(axis=1, func=lambda row: (row.proc_stats.stats\n",
    "            .groupby('pid')[['cpu_user', 'cpu_system', 'mem_rss', 'mem_vms']].agg(lambda g: g.max() - g.min())\n",
    "            .sum(axis=0)\n",
    "        ))\n",
    "        .rename(columns={\n",
    "            'cpu_user': 'cpu_user_time',\n",
    "            'cpu_system': 'cpu_system_time',\n",
    "            'mem_rss': 'mem_rss_delta',\n",
    "            'mem_vms': 'mem_vms_delta',\n",
    "        })\n",
    "    ))\n",
    "    .assign(\n",
    "        cpu_time=lambda df: df.cpu_user_time + df.cpu_system_time,\n",
    "        cpu_time_m=lambda df: df.cpu_time / 60,  # s -> m\n",
    "    )\n",
    "    # For xgb_rf, rf, ovr-rf\n",
    "    .assign(\n",
    "        c_multiclass=lambda df: df.params_dict.str.get('c_multiclass'),\n",
    "        c_max_depth=lambda df: df.params_dict.str.get('c_max_depth'),\n",
    "        # c_rf_max_depth=lambda df: df.apply(axis=1, func=lambda row: (\n",
    "        #     row['c_max_depth'] * (10 if row['c_multiclass'] == 'ovr' else 1)  # HACK Undo rf_max_depth -> ovr_rf_max_depth\n",
    "        # )),\n",
    "    )\n",
    "    # model_stats\n",
    "    .assign(\n",
    "        n_iters=lambda df: df.model_stats.map(lambda stats: or_else(None, lambda: stats.n_iter.tolist())),\n",
    "        forest_depth_mean=lambda df: df.model_stats.map(lambda stats: or_else(None, lambda: stats.depth.mean())),\n",
    "        forest_depth_std=lambda df: df.model_stats.map(lambda stats: or_else(None, lambda: stats.depth.std())),\n",
    "    )\n",
    "    # Reorder\n",
    "    .pipe(df_reorder_cols,\n",
    "        first=['model_id', 'params', 'params_dict', 'params_data', 'params_complexity', 'params_model'],\n",
    "        last=['train_evals', 'test_evals', 'proc_stats', 'model_stats', 'model'],\n",
    "    )\n",
    "    # .pipe(tap, f=lambda df: log.debug('display'))\n",
    "    .pipe(tap, f=lambda df: display(\n",
    "        # df_summary(df).T,\n",
    "        # df,\n",
    "        len(df),\n",
    "        df[:5],\n",
    "    ))\n",
    "    # .pipe(tap, f=lambda df: log.debug('Done'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.71s"
   },
   "outputs": [],
   "source": [
    "# Train/test scores\n",
    "#   - Grain: models x {train,test}\n",
    "#   - Metrics: score\n",
    "#   - Dims: group, params\n",
    "metrics = [\n",
    "    'train_mean_score', 'test_mean_score',\n",
    "    'train_50p_score', 'test_50p_score',\n",
    "    'train_75p_score', 'test_75p_score',\n",
    "    'train_95p_score', 'test_95p_score',\n",
    "]\n",
    "(cv_models\n",
    "    .pipe(df_reverse_cat, 'params_complexity')\n",
    "    .pipe(lambda df: pd.melt(df,\n",
    "        id_vars=[\n",
    "            'params', 'params_data', 'params_model', 'params_complexity',\n",
    "            'params_data_and_model', 'params_model_and_complexity',\n",
    "            'n_species', 'n_recs',\n",
    "            'fold',\n",
    "        ],\n",
    "        value_vars=metrics,\n",
    "        var_name='split_metric',\n",
    "        value_name='score'),\n",
    "    )\n",
    "    .assign(\n",
    "        split_metric=lambda df: df.split_metric.str.replace('_score', ''),\n",
    "        split=lambda df: df.split_metric.str.split('_').str[0],\n",
    "        metric=lambda df: df.split_metric.str.split('_').str[1],\n",
    "        group=lambda df: df.params_complexity.str.cat(df.split_metric, '/'),\n",
    "    )\n",
    "    .pipe(df_ordered_cat,\n",
    "        split_metric=[strip_endswith(x, '_score') for x in metrics],\n",
    "        group=lambda df: reversed(df.group.unique()),\n",
    "    )\n",
    "    .pipe(lambda df: (df\n",
    "        .pipe(ggplot)\n",
    "        + aes(x='params_complexity')\n",
    "        + aes(y='score')\n",
    "        + aes(color='metric')\n",
    "        # + facet_grid('params_data ~ params_model')\n",
    "        + facet + theme_\n",
    "        + geom_hline(yintercept=-1, color='lightgrey')  # -1 is the max score (1 is the min coverage_error)\n",
    "        + geom_point(df[df.split == 'train'], alpha=.3, fill='none')\n",
    "        + geom_point(df[df.split == 'test'], alpha=.8)\n",
    "        + geom_line(df[df.split == 'train'], alpha=.3, mapping=aes(group='split_metric', color='metric'))\n",
    "        + geom_line(df[df.split == 'test'], alpha=.8, mapping=aes(group='split_metric', color='metric'))\n",
    "        # TODO How to manually add a legend that shows test:filled, train:unfilled?\n",
    "        # + geom_jitter(fill='none', size=3, height=1e-9, width=.05)\n",
    "        # + geom_count(aes(size='..n..')) + scale_size_area()\n",
    "        # + stat_summary(aes(group='group'), fun_data='mean_cl_boot', random_state=0, geom='errorbar')\n",
    "        + coord_flip(\n",
    "            ylim=(-17, 0),\n",
    "        )\n",
    "        # + scale_y_continuous(breaks=np.arange(-10, 0))  # TODO TODO XXX\n",
    "        + scale_color_cmap_d('Set1')\n",
    "        + theme_figsize(width=figsize_width, aspect_ratio=1/1.5)\n",
    "        + ylab('score (-coverage_error)')\n",
    "        + ggtitle(f'Train/test scores ({recs_stats})')\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": false,
    "time": "1.42s"
   },
   "outputs": [],
   "source": [
    "(cv_models\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='cpu_time_m')\n",
    "    + aes(color='params_complexity')\n",
    "    + facet + theme_\n",
    "    + geom_line(aes(y='train_mean_score'), alpha=.3, color='darkgray')\n",
    "    + geom_line(aes(y='test_mean_score'), alpha=.8, color='darkgray')\n",
    "    + geom_point(aes(y='train_mean_score'), alpha=.3, fill='none')\n",
    "    + geom_point(aes(y='test_mean_score'), alpha=.8)\n",
    "    + geom_hline(yintercept=0, color='grey')\n",
    "    + expand_limits(x=0)\n",
    "    + coord_flip(\n",
    "        # + ylim(-30, 0)\n",
    "    )\n",
    "    + scale_color_cmap_d(mpl_cmap_concat('tab20', 'tab20b', 'tab20c'))\n",
    "    + guides(color=guide_legend(ncol=1)) + theme(legend_position='right', legend_box_spacing=.4, legend_key_height=8)\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1.5)\n",
    "    + ggtitle('mean_score ~ cpu_time_m')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "(cv_models\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='cpu_time_m')\n",
    "    + aes(color='params_complexity')\n",
    "    + facet + theme_\n",
    "    + geom_line(aes(y='train_75p_score'), alpha=.3, color='darkgray')\n",
    "    + geom_line(aes(y='test_75p_score'), alpha=.8, color='darkgray')\n",
    "    + geom_point(aes(y='train_75p_score'), alpha=.3, fill='none')\n",
    "    + geom_point(aes(y='test_75p_score'), alpha=.8)\n",
    "    + geom_hline(yintercept=0, color='grey')\n",
    "    + expand_limits(x=0)\n",
    "    + coord_flip(\n",
    "        # + ylim(-30, 0)\n",
    "    )\n",
    "    + scale_color_cmap_d(mpl_cmap_concat('tab20', 'tab20b', 'tab20c'))\n",
    "    + guides(color=guide_legend(ncol=1)) + theme(legend_position='right', legend_box_spacing=.4, legend_key_height=8)\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1.5)\n",
    "    + ggtitle('75p_score ~ cpu_time_m')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "(cv_models\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='model_size')\n",
    "    + aes(color='params_complexity')\n",
    "    + facet + theme_\n",
    "    + geom_line(aes(y='train_75p_score'), alpha=.3, color='darkgray')\n",
    "    + geom_line(aes(y='test_75p_score'), alpha=.8, color='darkgray')\n",
    "    + geom_point(aes(y='train_75p_score'), alpha=.3, fill='none')\n",
    "    + geom_point(aes(y='test_75p_score'), alpha=.8)\n",
    "    + geom_hline(yintercept=0, color='grey')\n",
    "    + scale_x_continuous(labels=labels_bytes(), breaks=breaks_bytes())\n",
    "    + expand_limits(x=0)\n",
    "    + coord_flip(\n",
    "        # + ylim(-30, 0)\n",
    "    )\n",
    "    + scale_color_cmap_d(mpl_cmap_concat('tab20', 'tab20b', 'tab20c'))\n",
    "    + guides(color=guide_legend(ncol=1)) + theme(legend_position='right', legend_box_spacing=.4, legend_key_height=8)\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1.5)\n",
    "    + ggtitle('test_75p_score ~ model_size')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": false,
    "time": "1.178s"
   },
   "outputs": [],
   "source": [
    "# Tune n_iter\n",
    "(cv_models\n",
    "    .merge(how='left', on='model_id', right=df_flatmap(cv_models, lambda row: (\n",
    "        dict(model_id=row.model_id, n_iter=n_iter)\n",
    "        for n_iter in coalesce(row.n_iters, [0])\n",
    "    )))\n",
    "    # Manually compute y.mean() per group\n",
    "    # .groupby('params').apply(lambda g: g.assign(cpu_time_m_mean=lambda df: df.cpu_time_m.mean()))\n",
    "    .pipe(df_reverse_cat, 'params_complexity')\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='params_complexity')\n",
    "    + facet + theme_\n",
    "    + geom_count(aes(y='n_iter', size='..n..')) + scale_size_area()\n",
    "    + expand_limits(y=0)\n",
    "    + coord_flip(\n",
    "        ylim=(0, 100),  # TODO\n",
    "    )\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1.5)\n",
    "    + ggtitle(f'n_iter ({recs_stats})')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": false,
    "time": "1.254s"
   },
   "outputs": [],
   "source": [
    "# Train time ~ n_species\n",
    "(cv_models\n",
    "    .pipe(df_reverse_cat, 'params_complexity')\n",
    "    .pipe(ggplot)\n",
    "    + facet_grid('-n_recs ~ params_model')\n",
    "    + aes(color='params_complexity')\n",
    "    + aes(x='n_species', y='cpu_time_m')\n",
    "    + geom_point()\n",
    "    + geom_line()\n",
    "    + scale_color_cmap_d(mpl_cmap_concat('tab20', 'tab20b', 'tab20c'))\n",
    "    + guides(color=guide_legend(ncol=1)) + theme(legend_position='right', legend_box_spacing=.4, legend_key_height=8)\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1.5)\n",
    "    + ggtitle(f'Train time ~ n_species ({recs_stats})')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": false,
    "time": "1.183s"
   },
   "outputs": [],
   "source": [
    "# Train time ~ n_recs\n",
    "(cv_models\n",
    "    .pipe(df_reverse_cat, 'params_complexity')\n",
    "    .pipe(ggplot)\n",
    "    + facet_grid('-n_species ~ params_model')\n",
    "    + aes(color='params_complexity')\n",
    "    + aes(x='n_recs', y='cpu_time_m')\n",
    "    + geom_point()\n",
    "    + geom_line()\n",
    "    + scale_color_cmap_d(mpl_cmap_concat('tab20', 'tab20b', 'tab20c'))\n",
    "    + guides(color=guide_legend(ncol=1)) + theme(legend_position='right', legend_box_spacing=.4, legend_key_height=8)\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1.5)\n",
    "    + ggtitle(f'Train time ~ n_recs ({recs_stats})')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.154s"
   },
   "outputs": [],
   "source": [
    "(cv_models\n",
    "    # Manually compute y.mean() per group\n",
    "    .groupby('params').apply(lambda g: g.assign(cpu_time_m_mean=lambda df: df.cpu_time_m.mean()))\n",
    "    .pipe(df_reverse_cat, 'params_complexity')\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='params_complexity')\n",
    "    + facet + theme_\n",
    "    + geom_col(aes(y='cpu_time_m_mean'), fill='darkgray', position=position_dodge())\n",
    "    + geom_point(aes(y='cpu_time_m'), color='black', fill='none', size=2)\n",
    "    + coord_flip()\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1.5)\n",
    "    + ggtitle(f'Train + score times ({recs_stats})')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": false,
    "time": "1.169s"
   },
   "outputs": [],
   "source": [
    "(cv_models\n",
    "    # Manually compute y.mean() per group\n",
    "    .groupby('params').apply(lambda g: g.assign(model_size_mean=lambda df: df.model_size.mean()))\n",
    "    .pipe(df_reverse_cat, 'params_complexity')\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='params_complexity')\n",
    "    + facet + theme_\n",
    "    + geom_col(aes(y='model_size_mean'), fill='darkgray', position=position_dodge()) # Summary per fold\n",
    "    + geom_point(aes(y='model_size'), color='black', fill='none', size=2) # Distribution of folds\n",
    "    + scale_y_continuous(labels=labels_bytes(), breaks=breaks_bytes())\n",
    "    + coord_flip()\n",
    "    + ylab('model_size')\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1.5)\n",
    "    + ggtitle(f'Model size ({recs_stats})')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# TODO Make not barf on non-tree models (blocks rest of notebook)\n",
    "(cv_models\n",
    "    # Manually compute boxplot stats, else it's _really_ slow (or you have to downsample, which misses outliers)\n",
    "    #   - Based on stat_boxplot.compute_group\n",
    "    .merge(how='left', on='params', right=cv_models.groupby('params').apply(lambda g: one(\n",
    "        pd.Series(dict(\n",
    "            params=g.name,\n",
    "            ymin=boxplot['whislo'],\n",
    "            lower=boxplot['q1'],\n",
    "            middle=boxplot['med'],\n",
    "            upper=boxplot['q3'],\n",
    "            ymax=boxplot['whishi'],\n",
    "            outliers=np.unique(boxplot['fliers']),  # np.unique else really slow, because lots of repeated (int) points\n",
    "        ))\n",
    "        for [boxplot] in [mpl.cbook.boxplot_stats(  # [boxplot] is 1 elem because X.ndim = 1\n",
    "            X=np.concatenate([[] if x is None else x.depth for x in g.model_stats]),\n",
    "            whis=1.5,\n",
    "        )]\n",
    "    )))\n",
    "    .pipe(df_reverse_cat, 'params_complexity')\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='params_complexity')\n",
    "    + facet + theme_\n",
    "    + geom_boxplot(\n",
    "        stat='identity',\n",
    "        outlier_size=.5,\n",
    "        mapping=aes(ymin='ymin', ymax='ymax', upper='upper', lower='lower', middle='middle', outliers='outliers',\n",
    "            width=.8,  # Close enough to geom_boxplot defaults [TODO Maybe should compute based on num categorical x's?]\n",
    "        ),\n",
    "    )\n",
    "    + ylab('tree_depth')\n",
    "    + coord_flip()\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1.5)\n",
    "    + ggtitle(f'Tree depth ({recs_stats})')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLOW_PLOTS_NEXT  # TODO(train_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model diagnostics: all models, all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('models:')\n",
    "print('  params[*/%s]' % len(cv_models.params.cat.categories))\n",
    "print('  fold[*/%s]' % cv.cv.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "21.278s"
   },
   "outputs": [],
   "source": [
    "# TODO Cache\n",
    "# Coverage errors: all models, all folds\n",
    "#   - Subset: all models\n",
    "#   - Grain: sum(recs[model.test_i] for model)\n",
    "#   - Dims: model_id, params, fold, y_true, rec_id\n",
    "#   - Metrics: coverage_error\n",
    "coverage_errors_all_all = (cv_models\n",
    "    # .sample(n=5, random_state=0)  # For faster dev\n",
    "    .pipe(lambda df: DF(\n",
    "        OrderedDict(\n",
    "            # **row[['model_id', 'params', 'fold']],  # Slow (in this inner loop), unpack manually instead\n",
    "            model_id=row.model_id,\n",
    "            params=row.params,\n",
    "            params_data=row.params_data,\n",
    "            params_model=row.params_model,\n",
    "            params_complexity=row.params_complexity,\n",
    "            params_data_and_model=row.params_data_and_model,\n",
    "            params_model_and_complexity=row.params_model_and_complexity,\n",
    "            fold=row.fold,\n",
    "            i=i,\n",
    "            y_true=y_true,\n",
    "            coverage_error=coverage_error,\n",
    "        )\n",
    "        for row in iter_progress(df_rows(df), n=len(df))\n",
    "        for i, y_true, coverage_error in zip(\n",
    "            row.test_evals.i,\n",
    "            row.test_evals.y,\n",
    "            row.test_evals.coverage_errors(),\n",
    "        )\n",
    "    ))\n",
    "    .astype(dict(\n",
    "        model_id=cv_models.model_id.dtype,\n",
    "        params=cv_models.params.dtype,\n",
    "        params_data=cv_models.params_data.dtype,\n",
    "        params_model=cv_models.params_model.dtype,\n",
    "        params_complexity=cv_models.params_complexity.dtype,\n",
    "        params_data_and_model=cv_models.params_data_and_model.dtype,\n",
    "        params_model_and_complexity=cv_models.params_model_and_complexity.dtype,\n",
    "    ))\n",
    "    .pipe(tap, lambda df: display(\n",
    "        df_summary(df).T,\n",
    "        df[:10],\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(x) for x in coverage_errors_all_all.params_data.unique()];\n",
    "params_data = (\n",
    "    'sp[331], recs[1.0]'\n",
    ")\n",
    "n_species = parse.search('sp[{n_species:d}], recs[{n_recs:f}]', params_data)['n_species']\n",
    "assert params_data in list(coverage_errors_all_all.params_data), params_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "66.201s"
   },
   "outputs": [],
   "source": [
    "# TODO Class imbalance\n",
    "#   1. Is class imbalance causing a problem? [-> maybe]\n",
    "#   2. Is class_weight solving it? [-> maybe a little bit]\n",
    "#   - TODO Try again with larger class imbalance\n",
    "(coverage_errors_all_all\n",
    "    [lambda df: df.params_data == params_data]\n",
    "    # .sample(100, random_state=0)  # Faster dev\n",
    "    .merge(how='left', on='y_true', right=(recs\n",
    "        .assign(n_recs=1).groupby('species')['n_recs'].sum().reset_index()\n",
    "        .rename(columns={'species': 'y_true'})\n",
    "    ))\n",
    "    # .pipe(puts, f=lambda df: df[:3])  # XXX Debug\n",
    "    .pipe(ggplot)\n",
    "    # + facet_grid('params_complexity ~ params_model')\n",
    "    + facet_wrap('params_model_and_complexity')\n",
    "    + aes(x='n_recs', y='coverage_error')\n",
    "    + geom_count(aes(size='..n..')) + scale_size_area()\n",
    "    + expand_limits(x=0, y=0)\n",
    "    + coord_flip()\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1)\n",
    "    + ggtitle(rf'Coverage error by n_recs per species ({recs_stats}) [{params_data}]')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "56.168s"
   },
   "outputs": [],
   "source": [
    "# Facet by params_no_ns, order by mean(coverage_error)\n",
    "#   - Subset: all models\n",
    "#   - Grain: sum(recs[model.test_i].groupby(params, y_true) for model)\n",
    "#       - Over: fold, rec_id\n",
    "#   - Dims: params, y_true\n",
    "#   - Metrics: coverage_error.mean\n",
    "# in: coverage_errors_all_all, recs\n",
    "(coverage_errors_all_all\n",
    "    [lambda df: df.params_data == params_data]\n",
    "    # .sample(100, random_state=0)  # Faster dev\n",
    "    # .pipe(df_reverse_cat, 'params', 'params_no_ns', 'ns')\n",
    "    .pipe(df_ordered_cat,\n",
    "        y_true=lambda df: (\n",
    "            # Sort species by mean(coverage_error) (across all models)\n",
    "            df.groupby('y_true').agg({'coverage_error': np.mean}).reset_index().sort_values('coverage_error').y_true\n",
    "            # Sort species by taxo (hard to compare across models, unless they're pretty low noise)\n",
    "            # reversed(recs.species.cat.categories)\n",
    "        ),\n",
    "    )\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='y_true', y='coverage_error')\n",
    "    + aes(color='y_true')\n",
    "    # + facet_grid('params_complexity ~ params_model')\n",
    "    + facet_wrap('params_model_and_complexity',\n",
    "        # Bug: dir='v' inverts nrow/ncol [https://github.com/has2k1/plotnine/issues/163]\n",
    "        # dir='v', nrow=coverage_errors_all_all.params_model.nunique(),\n",
    "    )\n",
    "    # + geom_line(aes(group='params'), stat='summary', fun_y=np.mean)  # TODO Bad interpolation with n_species\n",
    "    + geom_point(aes(group='params'), stat='summary', fun_y=np.mean)\n",
    "    + coord_flip()\n",
    "    # + geom_hline(yintercept=recs.species.nunique(), color='grey')\n",
    "    + scale_color_cmap_d(mpl_cmap_repeat(10, 'tab20', 'tab20b', 'tab20c'))\n",
    "    + theme_minimal_white()  # Before other theme()\n",
    "    + guides(color=guide_legend(nrow=70))\n",
    "    + theme(legend_position='right', legend_box_spacing=.4, legend_key_height=8, legend_text=element_text(size=8))\n",
    "    + theme(axis_text_y=element_blank())\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/2)\n",
    "    + ggtitle(rf'Coverage error over fold $\\times$ instance ({recs_stats}) [{params_data}]')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "155.551s"
   },
   "outputs": [],
   "source": [
    "# TODO Slow, bad for notebook dev loop -- move lower, or disable by default?\n",
    "# Facet by species\n",
    "#   - Subset: all models\n",
    "#   - Grain: sum(recs[model.test_i].groupby(params, y_true) for model)\n",
    "#       - Over: fold, rec_id\n",
    "#   - Dims: params, y_true\n",
    "#   - Metrics: coverage_error.mean\n",
    "# in: coverage_errors_all_all, recs\n",
    "[print(x) for x in coverage_errors_all_all.params_model.unique()];\n",
    "params_model = [\n",
    "    'ovr-logreg_ovr, liblinear, balanced',\n",
    "]\n",
    "(coverage_errors_all_all\n",
    "    [lambda df: df.params_data == params_data]\n",
    "    [lambda df: df.params_model.isin(params_model)]\n",
    "    # .sample(200, random_state=0)  # Faster dev\n",
    "    # [lambda df: df.y_true.isin(df.y_true.drop_duplicates().sample(n=3, random_state=0))]  # Faster dev\n",
    "    .astype({'y_true': metadata.species.df.shorthand.dtype})\n",
    "    # .pipe(df_reverse_cat, 'params_complexity')\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='params_complexity', y='coverage_error')\n",
    "    + aes(color='params_complexity')\n",
    "    + facet_wrap('y_true',\n",
    "        ncol=int((n_species * 1/(2/3)) ** .5),\n",
    "    )\n",
    "    + geom_hline(yintercept=1, color='grey')\n",
    "    # + geom_hline(yintercept=recs.species.nunique(), color='grey')\n",
    "    # Percentiles (faster, no overplot)\n",
    "    # + geom_point(stat='summary', fun_y=np.mean)\n",
    "    # + geom_linerange(stat='summary', fun_ymin=partial(np.percentile, q=25), fun_ymax=partial(np.percentile, q=75))\n",
    "    # Violin (slow, no overplot)\n",
    "    # + geom_violin()\n",
    "    # Boxplot (very slow, no overplot)\n",
    "    # + geom_boxplot()\n",
    "    # Points (medium cost, high overplot)\n",
    "    #   - n (count) instead of prop (proportion)\n",
    "    #   - scale_size_area() instead of default scale_size(), because it's a count [I don't grok this but it looks good]\n",
    "    + geom_count(aes(size='..n..'), color='lightgray')\n",
    "    + scale_size_area()\n",
    "    # + geom_point(stat='summary', fun_y=np.mean, shape='|', size=5, stroke=2)\n",
    "    + geom_point(stat='summary', fun_y=np.mean, size=5)\n",
    "    + coord_flip(\n",
    "        ylim=(0, 40),\n",
    "    )\n",
    "    # + scale_color_cmap_d(mpl_cmap_repeat(10, 'tab10'))  # Strong\n",
    "    + scale_color_cmap_d(mpl_cmap_repeat(1, 'tab20', 'tab20b', 'tab20c'))  # FIXME repeat(10) makes lots of blue/gray\n",
    "    + guides(color=guide_legend(reverse=True))\n",
    "    + theme(axis_text_y=element_text(size=6))\n",
    "    + theme_minimal_white()  # [TODO Before other theme()]\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1)\n",
    "    + ggtitle(rf'Coverage error over fold $\\times$ instance, by params_complexity ({recs_stats}) [{params_data}, {params_model}]')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "54.002s"
   },
   "outputs": [],
   "source": [
    "# TODO Slow, bad for notebook dev loop -- move lower, or disable by default?\n",
    "# Facet by species\n",
    "#   - Subset: all models\n",
    "#   - Grain: sum(recs[model.test_i].groupby(params, y_true) for model)\n",
    "#       - Over: fold, rec_id\n",
    "#   - Dims: params, y_true\n",
    "#   - Metrics: coverage_error.mean\n",
    "# in: coverage_errors_all_all, recs\n",
    "[print(x) for x in coverage_errors_all_all.params_complexity.unique()];\n",
    "params_complexity = [\n",
    "    # 'c_C[0.1]',\n",
    "    'c_C[0.001]',\n",
    "    # '',\n",
    "]\n",
    "(coverage_errors_all_all\n",
    "    [lambda df: df.params_data == params_data]\n",
    "    [lambda df: df.params_complexity.isin(params_complexity)]\n",
    "    # [lambda df: df.params.astype(str).str.contains(r'c_n_estimators\\[100\\]')]  # XXX Subset models\n",
    "    # .sample(200, random_state=0)  # Faster dev\n",
    "    # [lambda df: df.y_true.isin(df.y_true.drop_duplicates().sample(n=3, random_state=0))]  # Faster dev\n",
    "    .astype({'y_true': metadata.species.df.shorthand.dtype})\n",
    "    # .pipe(df_reverse_cat, 'params_model')\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='params_model', y='coverage_error')\n",
    "    + aes(color='params_model')\n",
    "    + facet_wrap('y_true',\n",
    "        ncol=int((n_species * 1/(2/3)) ** .5),\n",
    "    )\n",
    "    + geom_hline(yintercept=1, color='grey')\n",
    "    # + geom_hline(yintercept=recs.species.nunique(), color='grey')\n",
    "    # Percentiles (faster, no overplot)\n",
    "    # + geom_point(stat='summary', fun_y=np.mean)\n",
    "    # + geom_linerange(stat='summary', fun_ymin=partial(np.percentile, q=25), fun_ymax=partial(np.percentile, q=75))\n",
    "    # Violin (slow, no overplot)\n",
    "    # + geom_violin()\n",
    "    # Boxplot (very slow, no overplot)\n",
    "    # + geom_boxplot()\n",
    "    # Points (medium cost, high overplot)\n",
    "    #   - n (count) instead of prop (proportion)\n",
    "    #   - scale_size_area() instead of default scale_size(), because it's a count [I don't grok this but it looks good]\n",
    "    + geom_count(aes(size='..n..'), color='lightgray')\n",
    "    + scale_size_area()\n",
    "    # + geom_point(stat='summary', fun_y=np.mean, shape='|', size=5, stroke=2)\n",
    "    + geom_point(stat='summary', fun_y=np.mean, size=5)\n",
    "    + coord_flip(\n",
    "        ylim=(0, 40),\n",
    "    )\n",
    "    + scale_color_cmap_d(mpl_cmap_repeat(1, 'tab20', 'tab20b', 'tab20c'))  # FIXME repeat(10) makes lots of blue/gray\n",
    "    + guides(color=guide_legend(reverse=True))\n",
    "    + theme(axis_text_y=element_text(size=6))\n",
    "    + theme_minimal_white()  # [TODO Before other theme()]\n",
    "    + theme_figsize(width=figsize_width, aspect_ratio=1/1)\n",
    "    + ggtitle(rf'Coverage error over fold $\\times$ instance, by params_model ({recs_stats}) [{params_data}, {params_complexity}]')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_MODEL_PLOTS_NEXT  # TODO(train_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model diagnostics: one model, all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (\n",
    "    'n_species[331], n_recs[1.0], c_cls[ovr-logreg_ovr],c_solver[liblinear],c_C[0.001],c_class_weight[balanced]'\n",
    ")\n",
    "print(\"params:\\n%s\" % '\\n'.join('  %s: %r' % (i, x) for i, x in enumerate(cv_models.params.cat.categories)))\n",
    "print()\n",
    "print('models:')\n",
    "params_i = list(cv_models.params.cat.categories).index(params)\n",
    "print('  params[%s/%s]: %r' % (params_i, len(cv_models.params.cat.categories), params))\n",
    "print('  fold[*/%s]' % cv.cv.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage errors: one model, all folds\n",
    "#   - Subset: models.params == params\n",
    "#   - Grain: sum(recs[model.test_i] for model)\n",
    "#   - Dims: model_id, params, fold, y_true, rec_id\n",
    "#   - Metrics: coverage_error\n",
    "coverage_errors_one_all = (coverage_errors_all_all\n",
    "    [lambda df: df.params == params]  # One model, all folds\n",
    ")\n",
    "display(\n",
    "    df_summary(coverage_errors_one_all).T,\n",
    "    coverage_errors_one_all[:5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "25.836s"
   },
   "outputs": [],
   "source": [
    "# Coverage error by n_recs per species\n",
    "scale = [\n",
    "    # 'linear',\n",
    "    # 'logy',\n",
    "    'logy', 'logx',\n",
    "]\n",
    "summary_fun_y = (\n",
    "    # 'mean'\n",
    "    # '75p'\n",
    "    '80p'\n",
    "    # '90p'\n",
    ")\n",
    "summary_fun = lambda s: (\n",
    "    np.mean if s == 'mean' else\n",
    "    partial(np.percentile, q=parse.parse('{:g}p', s)[0])\n",
    ")\n",
    "logy_breaks=[1, 5, 10, 20, 30, 40, 50, 100, 200, 300]  # HACK Data dependent\n",
    "logx_breaks=[1, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 600, 700]  # HACK Data dependent\n",
    "for _params in [params]:\n",
    "    print(_params)\n",
    "    repr(coverage_errors_all_all\n",
    "        [lambda df: df.params == _params]  # One model, all folds\n",
    "        # .sample(n=100, random_state=0)  # XXX Faster dev\n",
    "        .merge(how='left', on='y_true', right=(recs\n",
    "            .assign(n_recs=1).groupby('species')['n_recs'].sum().reset_index()\n",
    "            .rename(columns={'species': 'y_true'})\n",
    "        ))\n",
    "        .merge(how='left', on='y_true', right=(metadata.species.df\n",
    "            [['shorthand', 'species_group', 'family', 'order']]\n",
    "            .rename(columns={'shorthand': 'y_true'})\n",
    "        ))\n",
    "        .astype({'y_true': metadata.species.df.shorthand.dtype})\n",
    "        .pipe(df_reverse_cat, 'y_true')\n",
    "        .pipe(df_remove_unused_categories)\n",
    "        .pipe(ggplot)\n",
    "        + theme_figsize('inline')\n",
    "        + aes(x='n_recs', y='coverage_error')\n",
    "        + geom_point(color='lightgray', size=.1)\n",
    "        + stat_summary(\n",
    "            fun_y=summary_fun(summary_fun_y),\n",
    "            geom='point',\n",
    "            mapping=aes(\n",
    "                group='y_true', color='species_group',  # Color by species_group\n",
    "                # group='y_true', color='y_true',  # Color by species\n",
    "            ),\n",
    "        )\n",
    "        + stat_summary(\n",
    "            fun_y=summary_fun(summary_fun_y),\n",
    "            geom='text', size=5,\n",
    "            mapping=aes(\n",
    "                group='y_true', label='y_true', color='species_group',  # Color by species_group\n",
    "                # group='y_true', color='y_true', label='y_true',  # Color by species\n",
    "                # group='y_true', color='y_true', label='''[  # Color by species, label only if f(x,y)\n",
    "                #     color if y >= 30 or x >= 300 else None  # (Great trick!)\n",
    "                #     for color, x, y in zip(..color.., ..x.., ..y..)\n",
    "                # ]''',\n",
    "            ),\n",
    "            nudge_y=4 if 'logy' not in scale else .02,\n",
    "        )\n",
    "        + geom_smooth(method='lm', se=True)\n",
    "        + theme_minimal_white()  # Before other theme()\n",
    "        + (geom_blank() if 'logy' not in scale else scale_y_log10(\n",
    "            breaks=logy_breaks,\n",
    "            labels=lambda breaks: [int(round(x)) for x in breaks],\n",
    "        ))\n",
    "        + (geom_blank() if 'logx' not in scale else scale_x_log10(\n",
    "            breaks=logx_breaks,\n",
    "            labels=lambda breaks: [int(round(x)) for x in breaks],\n",
    "        ))\n",
    "        + scale_color_cmap_d(mpl_cmap_with_colors('plasma_r', lambda colors: colors[256//8:]))  # Exclude the hard-to-see yellows\n",
    "        # + guides(color=False)\n",
    "        + guides(color=guide_legend(ncol=1))\n",
    "        + theme(legend_position='right', legend_key_height=8, legend_text=element_text(size=6), legend_entry_spacing=0)\n",
    "        + theme_figsize(width=18)\n",
    "        + ggtitle(f'{summary_fun_y} coverage error by n_recs per species ({recs_stats})\\n{_params}')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "32.633s"
   },
   "outputs": [],
   "source": [
    "#   - Subset: models.params == params\n",
    "#   - Grain: sum(recs[model.test_i].groupby(y_true) for model)\n",
    "#       - Over: fold, rec_id\n",
    "#   - Dims: y_true\n",
    "#   - Metrics: count, coverage_error.percentiles\n",
    "# in: coverage_errors_one_all, recs\n",
    "for _params in [params]:\n",
    "    print(_params)\n",
    "    repr(coverage_errors_all_all\n",
    "        [lambda df: df.params == _params]  # One model, all folds\n",
    "        .astype({'y_true': metadata.species.df.shorthand.dtype})\n",
    "        .pipe(df_reverse_cat, 'y_true')\n",
    "        .pipe(ggplot, aes(x='y_true', y='coverage_error'))\n",
    "        + geom_hline(yintercept=1, color='grey')\n",
    "        + geom_hline(yintercept=10, color='grey')\n",
    "        # + geom_hline(yintercept=recs.species.nunique(), color='grey')\n",
    "        + geom_count(aes(size='..n..'), alpha=1)  # n (count) instead of prop (proportion)\n",
    "        + scale_size_area()  # Instead of default scale_size(), because it's a count [I don't grok this but it looks good]\n",
    "        + geom_point(stat='summary', fun_y=np.mean, alpha=1, color='red', shape='|', size=6, stroke=2)\n",
    "        + coord_flip(\n",
    "            ylim=(1, n_species),\n",
    "        )\n",
    "        # + theme_figsize('inline')\n",
    "        # + theme_figsize('square')\n",
    "        # + theme_figsize('half')\n",
    "        + theme_figsize('half_dense')\n",
    "        # + theme_figsize('full')\n",
    "        # + theme_figsize('full_dense')\n",
    "        + ggtitle(f'Coverage error over fold $\\\\times$ instance ({recs_stats})\\n{_params}')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "4.046s"
   },
   "outputs": [],
   "source": [
    "# One-model/all-folds confusion matrix\n",
    "for _params in [params]:\n",
    "    print(_params)\n",
    "    with figsize(\n",
    "        # 'square',\n",
    "        # 'full',\n",
    "        'full_dense',\n",
    "    ):\n",
    "        repr(cv_models\n",
    "            [lambda df: df.params == _params]\n",
    "            .pipe(lambda df: plot_confusion_matrix(\n",
    "                classes=df.iloc[0].test_evals.classes,\n",
    "                M=np.array([\n",
    "                    row.test_evals.confusion_matrix_prob()\n",
    "                    for row in df_rows(df)\n",
    "                ]).sum(axis=0),\n",
    "                # normalize=False,  # For counts\n",
    "                raw=True, scale=10,  # Faster dev\n",
    "                format=None,  # Omit numbers, too dense\n",
    "                title=f'({recs_stats})',\n",
    "            ))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BORING_ONE_MODEL_ONE_FOLD_PLOTS_NEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model diagnostics: one model, one fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "fold = 0\n",
    "# params_i = ...  # Comment out to reuse from above\n",
    "params = cv_models.params.cat.categories[params_i]\n",
    "[(_, model)] = list(cv_models[lambda df: (df.params == params) & (df.fold == fold)].iterrows())\n",
    "print(\"params:\\n%s\" % '\\n'.join('  %s: %r' % (i, x) for i, x in enumerate(cv_models.params.cat.categories)))\n",
    "print()\n",
    "print('model:')\n",
    "print('  params[%s/%s]: %r' % (params_i, len(cv_models.params.cat.categories), model.params))\n",
    "print('  fold[%s/%s]' % (model.fold, cv.cv.n_splits))\n",
    "print()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# in: model\n",
    "model_id = model.model_id\n",
    "params = model.params\n",
    "fold = model.fold\n",
    "# train_evals = model.train_evals\n",
    "test_evals = model.test_evals\n",
    "\n",
    "# in: model, recs\n",
    "# train_recs = recs.iloc[train_evals.i]\n",
    "# train_X = Search.X(recs)[train_evals.i]\n",
    "# train_y = Search.y(recs)[train_evals.i]\n",
    "test_recs = recs.iloc[test_evals.i]\n",
    "test_X = Search.X(recs)[test_evals.i]\n",
    "test_y = Search.y(recs)[test_evals.i]  # (Don't need to store cv_models.test_evals.y if we have recs -- which sometimes we don't?)\n",
    "\n",
    "display(\n",
    "    # len(train_recs),\n",
    "    len(test_recs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# TODO Restore this plot like 'Coverage error over ...' above, so we can see _one_ model instead of aggregating over n_splits models\n",
    "# # TODO Update [kill the .merge, then species -> y_true]\n",
    "# (search.coverage_error_by(test_recs, 'id')\n",
    "#     [:5]\n",
    "#     # .merge(test_recs[['id', 'species']], on='id', how='left')\n",
    "#     # .pipe(ggplot, aes(x='species', y='coverage_error'))\n",
    "#     # + geom_count(aes(size='..n..'))\n",
    "#     # + stat_summary(fun_y=np.mean, geom='point', color='red', alpha=.5, shape='|', size=6, stroke=1)\n",
    "#     # + stat_summary(\n",
    "#     #     fun_ymin=partial(np.percentile, q=25), fun_ymax=partial(np.percentile, q=75),\n",
    "#     #     geom='linerange', color='red', alpha=.5, size=1,\n",
    "#     # )\n",
    "#     # + coord_flip()\n",
    "#     # + geom_hline(yintercept=len(search.classes_), color='grey')\n",
    "#     # + scale_x_discrete(limits=list(reversed(test_recs.species.cat.categories)))\n",
    "#     # + theme_figsize('square')\n",
    "#     # + ggtitle(rf'Coverage error over instance ({model_id}) ({recs_stats})')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# One-model/one-fold confusion matrix\n",
    "with figsize(\n",
    "    'square',\n",
    "    # 'full',\n",
    "    # 'full_dense',\n",
    "):\n",
    "    plot_confusion_matrix_df(\n",
    "        confusion_matrix_prob_df(model.test_evals.y, model.test_evals.y_scores, model.test_evals.classes),\n",
    "        title=model.model_id,\n",
    "        # normalize=False,  # For counts\n",
    "        raw=True, scale=10,  # Faster dev\n",
    "        title=f'({recs_stats})',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_PLOTS_NEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug plots, ignored by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "(cv_models\n",
    "    .pipe(df_reverse_cat, 'params', 'params_no_ns', 'ns')\n",
    "    # Manually compute y.mean() per group\n",
    "    .groupby('params').apply(lambda g: g.assign(mem_rss_delta_mean=lambda df: df.mem_rss_delta.mean()))\n",
    "    .pipe(ggplot, aes(x='ns', group='params_no_ns'))\n",
    "    + geom_col(aes(y='mem_rss_delta_mean', fill='params_no_ns'), position=position_dodge()) # Summary per fold\n",
    "    + geom_point(aes(y='mem_rss_delta'), fill='none', size=2, position=position_dodge(width=.9)) # Distribution of folds\n",
    "    + coord_flip()\n",
    "    + scale_fill_cmap_d(mpl_cmap_concat('tab20', 'tab20b', 'tab20c'))\n",
    "    + scale_y_continuous(labels=labels_bytes(), breaks=breaks_bytes())\n",
    "    + guides(fill=guide_legend(reverse=True))\n",
    "    + theme(legend_position='bottom', legend_direction='vertical', legend_box_spacing=.4, legend_key_height=8)\n",
    "    + theme_figsize(aspect_ratio=1/3*2)\n",
    "    + ggtitle(f'Mem rss spread ($max-min$) ({recs_stats})')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# TODO Very slow with ~200 models\n",
    "cv_models_proc_stats = (cv_models\n",
    "    # Enable these as needed (at the cost of mem usage)\n",
    "    [[\n",
    "        'model_id',\n",
    "        'params',\n",
    "        # 'params_dict',\n",
    "        'params_data',\n",
    "        'params_complexity',\n",
    "        'params_model',\n",
    "        'fold',\n",
    "        # 'train_score',\n",
    "        # 'test_score',\n",
    "        # 'fit_time',\n",
    "        # 'score_time',\n",
    "        'mem_rss_delta',\n",
    "        # 'mem_vms_delta',\n",
    "        # 'train_evals',\n",
    "        # 'test_evals',\n",
    "        'proc_stats',\n",
    "    ]]\n",
    "    # TODO Faster way to do this? (.merge is >2x slower)\n",
    "    .pipe(df_flatmap, lambda row: [\n",
    "        row.append(pd.Series(dict(**stats)))\n",
    "        for stats in row.proc_stats.stats\n",
    "    ])\n",
    "    # .pipe(lambda df: (df\n",
    "    #     .merge(how='left',\n",
    "    #         right=DF(\n",
    "    #             OrderedDict(model_id=row.model_id, **stats)\n",
    "    #             for row in df_rows(df)\n",
    "    #             for stats in row.proc_stats.stats\n",
    "    #         )\n",
    "    #     )\n",
    "    # ))\n",
    "    # HACK Restore the cats that the df_flatmap just destroyed [copied from cv_results_splits_df]\n",
    "    .pipe(df_ordered_cat,\n",
    "        model_id=lambda df: df.model_id.unique(),\n",
    "        params=lambda df: df.params.unique(),\n",
    "    )\n",
    "    .assign(\n",
    "        cpu_user=lambda df: df.cpu_user.diff() * 100,\n",
    "        cpu_system=lambda df: df.cpu_system.diff() * 100,\n",
    "    )\n",
    "    .pipe(tap, f=lambda df: display(\n",
    "        df_summary(df).T,\n",
    "        df[:5],\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# TODO Might need to stack/geom_area for overlapping runs\n",
    "(cv_models_proc_stats\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='time', color='model_id')\n",
    "    + geom_line(aes(y='cpu_system'), linetype='dashed', size=.5)\n",
    "    + geom_line(aes(y='cpu_system + cpu_user'), linetype='solid', size=.5)\n",
    "    + expand_limits(y=0)\n",
    "    + ylab('cpu')\n",
    "    + scale_x_datetime(date_labels='%H:%M:%S')\n",
    "    + scale_y_continuous(labels=lambda labels: ['%.3g%%' % x for x in labels])\n",
    "    + theme(legend_position='bottom', legend_direction='vertical', legend_box_spacing=.4, legend_key_height=8)\n",
    "    + theme_figsize('inline_short')\n",
    "    + ggtitle('cpu over time (system + user)')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "(cv_models_proc_stats\n",
    "    .pipe(ggplot)\n",
    "    + aes(x='time', color='model_id')\n",
    "    + geom_line(aes(y='mem_rss'), linetype='solid', size=.5)\n",
    "    + geom_line(aes(y='mem_vms'), linetype='dashed', size=.5)\n",
    "    + expand_limits(y=0)\n",
    "    + ylab('mem')\n",
    "    + scale_x_datetime(date_labels='%H:%M:%S')\n",
    "    + scale_y_continuous(labels=labels_bytes(), breaks=breaks_bytes(pow=3))\n",
    "    + theme(legend_position='bottom', legend_direction='vertical', legend_box_spacing=.4, legend_key_height=8)\n",
    "    + theme_figsize('inline_short')\n",
    "    + ggtitle('Mem over time (rss, vms)')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: RF tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "skip": true
   },
   "outputs": [],
   "source": [
    "# TODO Revive if insightful\n",
    "(cv_models\n",
    "    .pipe(df_reverse_cat, 'params_no_ns')\n",
    "    .assign(n_species=lambda df: df.params_dict.str['n_species'])\n",
    "    .pipe(ggplot, aes(x='n_species', y='forest_depth_mean', color='params_no_ns'))\n",
    "    # + facet_wrap('params_no_ns', ncol=2)\n",
    "    + geom_point()\n",
    "    + geom_pointrange(aes(ymin='forest_depth_mean - 2*forest_depth_std', ymax='forest_depth_mean + 2*forest_depth_std'))\n",
    "    + geom_smooth(method='lm', se=False)  # Disable se because it only knows forest_depth_mean, no measure of spread\n",
    "    + expand_limits(x=0)\n",
    "    + scale_color_cmap_d(mpl_cmap_concat('tab20', 'tab20b', 'tab20c'))\n",
    "    + guides(color=guide_legend(reverse=True))\n",
    "    + theme(legend_position='bottom', legend_direction='vertical', legend_box_spacing=.4, legend_key_height=8)\n",
    "    # + theme_figsize(aspect_ratio=1/3)\n",
    "    + theme_figsize(aspect_ratio=1/3)\n",
    "    + ggtitle(f'RF depth vs. n_species ({recs_stats})')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
