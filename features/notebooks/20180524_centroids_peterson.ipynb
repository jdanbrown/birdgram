{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from potoo.plot import *\n",
    "from potoo.util import *\n",
    "import sklearn\n",
    "\n",
    "from cache import *\n",
    "from constants import *\n",
    "from datasets import *\n",
    "from features import *\n",
    "from load import *\n",
    "from sp14.model import *\n",
    "from util import *\n",
    "from viz import *\n",
    "\n",
    "figsize('inline_short');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_paths = load_recs_paths(['peterson-field-guide'])\n",
    "display(\n",
    "    df_summary(recs_paths),\n",
    "    recs_paths[:5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = load_recs_data(recs_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    df_summary(recs),\n",
    "    recs[:5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(recs\n",
    "    .pipe(df_reverse_cat, 'species_longhand')\n",
    "    .pipe(ggplot, aes(x='species_longhand'))\n",
    "    + geom_bar()\n",
    "    + coord_flip()\n",
    "    + ylab('num recordings')\n",
    "    + theme_figsize(aspect_ratio=1/2)\n",
    "    + ggtitle('How many training recordings per species?')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['n_recs', 'n_xc_recs']:\n",
    "    repr(recs\n",
    "        .assign(\n",
    "            n_recs=1,\n",
    "            species=lambda df: df.species.cat.remove_unused_categories(),  # Else groupby includes all categories\n",
    "            species_longhand=lambda df: df.species_longhand.cat.remove_unused_categories(),  # Else groupby includes all categories\n",
    "        )\n",
    "        .groupby(['species', 'species_longhand'])[['n_recs']].sum().reset_index()\n",
    "        .dropna()\n",
    "        .merge(\n",
    "            metadata.xc_counts.with_species,\n",
    "            how='left',\n",
    "            left_on='species',\n",
    "            right_on='shorthand',\n",
    "        )\n",
    "        .pipe(pd.melt, id_vars=['species', 'species_longhand'], value_vars=['n_recs', 'n_xc_recs'])\n",
    "        .astype({\n",
    "            'species': recs.species.dtype,\n",
    "            'species_longhand': recs.species_longhand.dtype,\n",
    "        })\n",
    "        .pipe(df_reverse_cat, 'species', 'species_longhand')\n",
    "        [lambda df: df.variable == col]  # XXX Workaround for facet_grid(scales='free') not working\n",
    "        .pipe(ggplot, aes(x='species_longhand', y='value'))\n",
    "        + geom_col()\n",
    "        + coord_flip()\n",
    "        # + facet_grid('. ~ variable', scales='free')  # FIXME Why doesn't scales='free' work here?\n",
    "        + ylab('num recordings')\n",
    "        + theme_figsize(aspect_ratio=1/2)\n",
    "        + ggtitle('How many training recordings per species?')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats\n",
    "(recs\n",
    "    .assign(\n",
    "        n=1,\n",
    "        duration_h=lambda df: df.duration_s / 3600,\n",
    "        samples_gb=lambda df: df.samples_mb / 1024,\n",
    "        species=lambda df: df.species.cat.remove_unused_categories(),  # Else groupby includes all categories\n",
    "    )\n",
    "    .groupby([\n",
    "        'dataset',\n",
    "        'species',\n",
    "    ])\n",
    "    [['n', 'duration_h', 'samples_gb', 'samples_n']]\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_test = recs\n",
    "display(\n",
    "    df_summary(recs_test),\n",
    "    recs_test[:10],\n",
    "    (recs_test\n",
    "        .assign(n=1)\n",
    "        .groupby(['dataset', 'species'])\n",
    "        ['n'].sum()\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectros\n",
    "model = Model(verbose_config=False)\n",
    "display(len(recs_test))\n",
    "recs['spectro'] = model.spectros(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with with_figsize(width=24, aspect_ratio=1/12):\n",
    "    for spectro in recs.query(\"species == 'OCWA'\").spectro[:3]:\n",
    "        spectro.plot(show_audio=True, fancy=True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with with_figsize(width=25/2, height=212/2):\n",
    "    plot_many_spectros(t_max=30, recs=(recs\n",
    "        [-50:]  # Avoid heavy plot\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(exp, pca_rows=3, centroid_rows=6):\n",
    "    display(exp.config.proj_skm_config)\n",
    "    with with_figsize('full'):\n",
    "        gs = mpl.gridspec.GridSpec(nrows=2, ncols=1, height_ratios=[1, 2], hspace=.1)\n",
    "        plt.subplot(gs[0, 0])\n",
    "        exp.model.plot_patches(exp.model.proj_skm_.pca.components_.T, rows=pca_rows)\n",
    "        plt.subplot(gs[1, 0])\n",
    "        exp.model.plot_proj_centroids(rows=centroid_rows, sort=dict(reverse=True, key=lambda patch: (\n",
    "            # patch.reshape(f, p).mean(axis=1).argmax()  # Freq mode\n",
    "            # patch.std()  # Less useful than .sum()\n",
    "            patch.sum()  # Spread (via total energy, which happens to correlate)\n",
    "        )))\n",
    "        plt.xlabel(str(exp.config.proj_skm_config))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sp14.model import *\n",
    "# k, n = 50,  60   # Faster dev\n",
    "# k, n = 50,  300  # [Useful?]\n",
    "k, n = 500, 300  # Mem safe (~17m uncached for 6 configs)\n",
    "# k, n = 500, 535  # Full [XXX Not mem safe] [len(recs) is currently 535]\n",
    "exps = []\n",
    "for i, config in enumerate([\n",
    "    # Experiment ordering:\n",
    "    #   - Order from least to most useful looking (determined mainly by high->low pca dimension)\n",
    "    #   - Interleave pca False->True since that's a very salient change to observe\n",
    "    Dict(n=n, proj_skm_config=dict(k=k, normalize=True,  standardize=True,  pca_whiten=False, do_pca=False)),\n",
    "    Dict(n=n, proj_skm_config=dict(k=k, normalize=True,  standardize=True,  pca_whiten=True,  do_pca=True)),\n",
    "    Dict(n=n, proj_skm_config=dict(k=k, normalize=True,  standardize=False, pca_whiten=False, do_pca=False)),\n",
    "    Dict(n=n, proj_skm_config=dict(k=k, normalize=True,  standardize=False, pca_whiten=True,  do_pca=True)),\n",
    "    Dict(n=n, proj_skm_config=dict(k=k, normalize=False, standardize=True,  pca_whiten=False, do_pca=False)),\n",
    "    Dict(n=n, proj_skm_config=dict(k=k, normalize=False, standardize=True,  pca_whiten=True,  do_pca=True)),\n",
    "    Dict(n=n, proj_skm_config=dict(k=k, normalize=False, standardize=False, pca_whiten=False, do_pca=False)),\n",
    "    Dict(n=n, proj_skm_config=dict(k=k, normalize=False, standardize=False, pca_whiten=True,  do_pca=True)),  # SKM defaults\n",
    "]):\n",
    "    print(f'\\n\\ni[{i}] config[{config}]\\n')\n",
    "    model = Model(\n",
    "        verbose_config=False,\n",
    "        proj_skm_config=config.proj_skm_config,\n",
    "    )\n",
    "    recs_test_n = (recs_test\n",
    "        .pipe(sklearn.utils.shuffle, random_state=0)\n",
    "        .sample(config.n, random_state=0)\n",
    "    )\n",
    "    model.fit_proj(recs_test_n)\n",
    "\n",
    "    # model.fit_class(recs_test_n)\n",
    "    # display(\n",
    "    #     model.test(recs_test_n, 'classes'),\n",
    "    #     model.test(recs_test_n, 'kneighbors'),\n",
    "    # )\n",
    "\n",
    "    exp = Dict(\n",
    "        i=i,\n",
    "        config=config,\n",
    "        model=model,\n",
    "    )\n",
    "    exps.append(exp)\n",
    "\n",
    "    # plot_results(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in exps:\n",
    "    plot_results(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "- Normalize and standardize are both junk (on top of the denoising we already have):\n",
    "    - With pca, all 3 combos of norm/std increase the intrinsic dimensionality of the data, which is counterproductive\n",
    "    - None of the 3 combos of norm/std make more visually plausible centroids than without\n",
    "- PCA whitening is helping:\n",
    "    - The “noise” centroids disappear with pca enabled\n",
    "- Woohoo!\n",
    "\n",
    "# Open Qs\n",
    "- Why does [SP14] Fig 10 (below) have some \"negative\" centroids (lots of black, little white), whereas we have none?\n",
    "    - Naively, I'd think that negative centroids aren't helpful, and that they are maybe due to noisy training data\n",
    "    - But it's not clear what kind of training data would produce them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [SP14] Fig 10\n",
    "Image(url='https://user-images.githubusercontent.com/627486/40503140-fc125570-5f41-11e8-9b82-e8abd5c129fb.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bubo-features (PYTHONSTARTUP)",
   "language": "python",
   "name": "bubo-features (PYTHONSTARTUP)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
