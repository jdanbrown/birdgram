{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "25.715s"
   },
   "outputs": [],
   "source": [
    "from notebooks import *\n",
    "memory.log.level = 'debug'\n",
    "sg.init(app=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster(/smaller) search_recs load from disk\n",
    "- See takeaways at bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "1.29s"
   },
   "outputs": [],
   "source": [
    "# Inspect types + values\n",
    "(sg.search_recs\n",
    "    [:1].T\n",
    "    .pipe(df_assign_first, type=lambda df: df[0].map(lambda x: type(x).__name__))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "7.463s"
   },
   "outputs": [],
   "source": [
    "# How big are f_*?\n",
    "#   - float16 or float32? -- see takeaways below\n",
    "print(f'shape: {sg.search_recs.shape}')\n",
    "for dtype in [\n",
    "    None,        # total[703m] = no_f_*[116m] + f_*[587m] (= f_f[479m] + f_p[108m]) -- List[float]\n",
    "    np.float64,  # total[639m] = no_f_*[116m] + f_*[523m] (= f_f[426m] + f_p[ 97m]) -- Same as np.float (np default)\n",
    "    np.float32,  # total[381m] = no_f_*[116m] + f_*[265m] (= f_f[215m] + f_p[ 50m])\n",
    "    np.float16,  # total[252m] = no_f_*[116m] + f_*[136m] (= f_f[109m] + f_p[ 27m])\n",
    "]:\n",
    "    mem = (sg.search_recs\n",
    "        # [:1000]  # XXX Faster dev\n",
    "        .pipe(lambda df: df if dtype is None else (df\n",
    "            .pipe(df_col_map,\n",
    "                f_f=lambda xs: np.array(xs, dtype=dtype),  # List[float] -> np.ndarray\n",
    "                f_p=lambda xs: np.array(xs, dtype=dtype),  # List[float] -> np.ndarray\n",
    "            )\n",
    "        ))\n",
    "        .memory_usage(deep=True)\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    print()\n",
    "    print(f'dtype: {dtype}')\n",
    "    print(f'total: {humanize.naturalsize(mem.sum())}')\n",
    "    display(mem\n",
    "        .to_frame().rename(columns={0: 'mem'})\n",
    "        .sort_values('mem', ascending=True)\n",
    "        .assign(rev_cumsum=lambda df: df.mem.cumsum())\n",
    "        .sort_values('mem', ascending=False)\n",
    "        .assign(cumsum=lambda df: df.mem.cumsum())\n",
    "        .applymap(humanize.naturalsize)\n",
    "        [:5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "time": "12.616s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.096] /tmp/df-5-352-float16.pkl write                        "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[  -5.0 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[ 17.3 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.069] /tmp/df-5-352-float16.pkl read                         "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[  12.3 kB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[ 17.3 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.083] /tmp/df-5-352-float16.parquet.uncompressed write       "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[  10.2 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[  6.2 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.018] /tmp/df-5-352-float16.parquet.uncompressed read        "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[      0 B]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[  6.2 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.456] /tmp/df-5-352-float16.sqlite write                     "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[   9.8 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[  6.1 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.035] /tmp/df-5-352-float16.sqlite read                      "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[      0 B]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[  6.1 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.106] /tmp/df-5-352-float32.pkl write                        "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[  -9.2 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[ 21.2 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.103] /tmp/df-5-352-float32.pkl read                         "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[  28.7 kB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[ 21.2 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.082] /tmp/df-5-352-float32.parquet.uncompressed write       "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[   9.1 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[  7.5 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.019] /tmp/df-5-352-float32.parquet.uncompressed read        "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[  32.8 kB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[  7.5 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.516] /tmp/df-5-352-float32.sqlite write                     "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[   7.6 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[  7.4 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "time[00:00.039] /tmp/df-5-352-float32.sqlite read                      "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "uss[ 278.5 kB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " file[  7.4 MB]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": [
    "# Perf (results recorded in next cell)\n",
    "def measure(path, msg, f):\n",
    "    # Various contortions to print time + mem delta + file size on one line\n",
    "    with print_mem_delta(\n",
    "        collect_before=True, collect_after=False,\n",
    "        desc=None,\n",
    "        format='uss[%(uss)9s]',  # uss > rss,vms [https://psutil.readthedocs.io/en/latest/#psutil.Process.memory_full_info]\n",
    "        print=partial(print, end='')\n",
    "    ):\n",
    "        # Retain ret to ensure mem delta is measured before it's collected\n",
    "        ret = timed_print(f=f, msg='time[%%s] %-55s' % f'{path} {msg}', print=partial(print, end=''))\n",
    "    print(' file[%8s]' % naturalsize(Path(path).stat().st_size))\n",
    "    return ret\n",
    "\n",
    "run = AttrDict(\n",
    "\n",
    "    # Params\n",
    "    version=5,\n",
    "    n=(\n",
    "        len(sg.search_recs) // 100\n",
    "        # len(sg.search_recs) // 10\n",
    "        # len(sg.search_recs)\n",
    "    ),\n",
    "    array_dtypes=[\n",
    "        np.float16,\n",
    "        np.float32,\n",
    "    ],\n",
    "\n",
    "    # File formats\n",
    "    pkl=None,  # Slow\n",
    "    parquet=[\n",
    "        'uncompressed',  # Fast\n",
    "        # 'gzip',  # Slow\n",
    "        # 'snappy',  # Not installed [should be faster than gzip]\n",
    "    ],\n",
    "    sqlite=None,  # Fast\n",
    "\n",
    ")\n",
    "\n",
    "for array_dtype in run.array_dtypes:\n",
    "    path = f'/tmp/df-{run.version}-{run.n}-{array_dtype.__name__}'\n",
    "    df = (sg.search_recs\n",
    "        # Limit (for faster dev)\n",
    "        [:run.n]\n",
    "        # HACK Drop df_cell cols (currently just *_stack)\n",
    "        #   - TODO Format the ones that matter as str/bytes (e.g. html)\n",
    "        [lambda df: [c for c in df if not c.endswith('_stack')]]\n",
    "        # ~10x faster to serdes np.array than list (but only slightly more compact)\n",
    "        .pipe(df_col_map,\n",
    "            f_f=lambda xs: np.array(xs, dtype=array_dtype),  # List[float] -> np.ndarray\n",
    "            f_p=lambda xs: np.array(xs, dtype=array_dtype),  # List[float] -> np.ndarray\n",
    "        )\n",
    "        # Convert types for serdes\n",
    "        .pipe(df_col_map,\n",
    "            feat=np_to_npy_bytes,         # np.array -> bytes (.parquet, .sqlite)\n",
    "            f_f=np_to_npy_bytes,          # np.array -> bytes (.parquet, .sqlite)\n",
    "            f_p=np_to_npy_bytes,          # np.array -> bytes (.parquet, .sqlite)\n",
    "            background=','.join,          # List[str] -> str (.sqlite)\n",
    "            background_species=','.join,  # List[str] -> str (.sqlite)\n",
    "        )\n",
    "    )\n",
    "    if 'pkl' in run:\n",
    "        pkl_path = f'{path}.pkl'\n",
    "        measure(pkl_path, 'write', lambda: joblib.dump(df, pkl_path))\n",
    "        measure(pkl_path, 'read',  lambda: joblib.load(pkl_path))\n",
    "    if 'parquet' in run:\n",
    "        for compression in run.parquet:\n",
    "            parquet_path = f'{path}.parquet.{compression}'\n",
    "            measure(parquet_path, 'write', lambda: df.to_parquet(parquet_path, engine='fastparquet', compression=compression))\n",
    "            measure(parquet_path, 'read',  lambda: pd.read_parquet(parquet_path, engine='fastparquet'))\n",
    "    if 'sqlite' in run:\n",
    "        sqlite_path = f'{path}.sqlite'\n",
    "        eng = sqla.create_engine(f'sqlite:///{sqlite_path}')\n",
    "        with eng.begin() as con:\n",
    "            measure(sqlite_path, 'write', lambda: df.to_sql('df', con=con, if_exists='replace',\n",
    "                chunksize=1000,  # Else mem unsafe (default is to write all rows in same operation)\n",
    "                # schema=..., # Any gains to be had here?\n",
    "                #   - f_* are TEXT instead of BLOB, but:\n",
    "                #       - TEXT accepts BLOB data as is [https://sqlite.org/datatype3.html#type_affinity]\n",
    "                #       - (Sanity check) Size in sqlite looks fine: n_elems * dtype_bytes + 128b (constant overhead)\n",
    "                #   - No other cols matter for size, so the rest will just be data fidelity and client compat\n",
    "            ))\n",
    "            measure(sqlite_path, 'read',  lambda: pd.read_sql_table('df', con=con))\n",
    "        eng.dispose()  # Release conn pools\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perf (for prev. cell)\n",
    "```\n",
    ".pkl                   n[35231] d[float16] file[1.7g] w[ 11s] r[7.0s] uss[904m]\n",
    ".parquet.uncompressed  n[35231] d[float16] file[607m] w[2.7s] r[1.0s] uss[304m]\n",
    ".parquet.gzip          n[35231] d[float16] file[530m] w[ 49s] r[8.7s] uss[405m]\n",
    ".sqlite                n[35231] d[float16] file[610m] w[ 38s] r[1.6s] uss[265m]\n",
    "\n",
    ".pkl                   n[35231] d[float32] file[2.1g] w[ 13s] r[ 10s] uss[1.1g]\n",
    ".parquet.uncompressed  n[35231] d[float32] file[736m] w[3.4s] r[1.2s] uss[384m]\n",
    ".parquet.gzip          n[35231] d[float32] file[652m] w[ 58s] r[ 11s] uss[503m]\n",
    ".sqlite                n[35231] d[float32] file[740m] w[ 37s] r[2.9s] uss[389m]\n",
    "```\n",
    "\n",
    "# Takeaways\n",
    "- Forget .pkl\n",
    "    - ~3x bigger, ~4–8x slower to read (vs. .sqlite/.parquet)\n",
    "- Forget .gzip\n",
    "    - ~10x slowdown\n",
    "    - TODO Revisit compression once we start sending payloads to mobile clients.\n",
    "- Start with float32 instead of float16\n",
    "    - Size: only ~1.2x bigger (.sqlite, .parquet) -- I was expecting ~2x\n",
    "    - Read time: ~1.8x for .sqlite, ~1.2x for .parquet\n",
    "    - TODO Revisit float16 later when we need ~1.2–2x gains\n",
    "- Start building on .sqlite, since .parquet is a dead end for mobile\n",
    "    - Size: same (< 1%)\n",
    "    - Read time: .sqlite ~2x slower than .parquet\n",
    "    - Write time: .sqlite ~10x slower than .parquet, but not a relevant bottleneck, and dominated by etl compute anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
